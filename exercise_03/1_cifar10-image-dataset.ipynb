{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CIFAR-10: Image Dataset\n",
    "\n",
    "Throughout this course, we will teach you all basic skills and how to use all neccessary tools that you need to implement deep neural networks, which is the main focus of this class. However, you should also be proficient with handling data and know how to prepare it for your specific task. In fact, most of the jobs that involve deep learning in industry are very data related, so this is an important skill that you have to pick up.\n",
    "\n",
    "Therefore, we will take a deep dive into data preparation this week by implementing our own datasets and dataloader. In this notebook, we will focus on the image dataset CIFAR-10. The CIFAR-10 dataset consists of 50000x3x32x32  colour (RGB) images: 50,000 images, 3 channels (RGB), and 32x32 pixels. Those images are clustered into 10 classes: *plane*, *car*, *bird*, *cat*, *deer*, *dog*, *frog*, *horse*, *ship*, *truck*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## (Optional) Mount in Google Colab\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you have created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: Terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_03) is given.\n",
    "# NOTE 3: To use, remove the triple \" from the beginning and end of this cell.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_03'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start by importing some libraries that you will need along the way, as well as some code files that you will work on throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from exercise_code.data import (\n",
    "    ImageFolderDataset,\n",
    ")\n",
    "from exercise_code.tests import (\n",
    "    test_rescale_transform,\n",
    "    test_compute_image_mean_and_std,\n",
    "    test_len_dataset,\n",
    "    test_item_dataset,\n",
    "    test_transform_dataset,\n",
    "    test_normalization_transform,\n",
    "    save_pickle\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Dataset Download\n",
    "Let us get started by downloading the data. In `exercise_code/data/image_folder_dataset.py` you can find a class `ImageFolderDataset`, which you will have to complete throughout this notebook.\n",
    "\n",
    "This class automatically downloads the raw data for you. To do so, simply initialize the class as below.\n",
    "\n",
    "**Note** Downloading and zipping the dataset might take a while, ESPECIALLY if you are using Google Colab. So be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working with colab uncomment the following line:\n",
    "\n",
    "### colab only ###\n",
    "# import urllib.request\n",
    "### colab only ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the output dataset folder\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")\n",
    "\n",
    "# Init the dataset and display downloading information this one time\n",
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    force_download=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should now be able to see the images in `i2dl_exercises/datasets/cifar10` in your file browser, which should contain one subfolder per class, each containing the respective images labeled `0001.png`, `0002.png`, ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By default, the dataset will only be downloaded the first time you initialize a dataset class. If, for some reason, your version of the dataset gets corrupted and you wish to re-download it, simply initialize the class with `force_download=True` in the download cell above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important notice for colab:**\n",
    "\n",
    "By running the above cell, the dataset is mounted to the runtime you are currently using. But it takes time to save the dataset to your google drive. The dataset might not be complete saved on your google drive due to many reason. When you restart the runtime or try to access cifar10 from another notebook, you may got error. When this happens, run the following cell in colab to redownload and extract the dataset for the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only when you got error regarding cifar10 in colab ###\n",
    "# !mkdir -p /content/gdrive/MyDrive/i2dl/datasets/cifar10\n",
    "# !wget -P /content/gdrive/MyDrive/i2dl/datasets/cifar10/ https://i2dl.vc.in.tum.de/static/data/cifar10.zip\n",
    "# !unzip -o /content/gdrive/MyDrive/i2dl/datasets/cifar10/cifar10.zip -d /content/gdrive/MyDrive/i2dl/datasets/cifar10\n",
    "# !rm -f /content/gdrive/MyDrive/i2dl/datasets/cifar10/cifar10.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Data Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before training any model you should *always* take a look at some samples of your dataset. In this way, you can make sure that the data input has worked as intended and also get a feeling for the dataset. \n",
    "\n",
    "Let's load the CIFAR-10 data and visualize a subset of the images. To do so, `PIL.Image.open()` is used to open an image, and then `numpy.asarray()` to cast the image to a numpy array, which will have shape 32x32x3 (Note that it is different from the default shape format used in this course, CxHxW). In this way 7 images will be loaded per class, and then use `matplotlib.pyplot` to visualize those images in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_image_as_numpy(image_path):\n",
    "    return np.asarray(Image.open(image_path), dtype=float)\n",
    "\n",
    "classes = [\n",
    "    'plane', 'car', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck',\n",
    "]\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for label, cls in enumerate(sorted(classes)):\n",
    "    for i in range(samples_per_class):\n",
    "        image_path = os.path.join(\n",
    "            cifar_root,\n",
    "            cls,\n",
    "            str(i+10).zfill(4) + \".png\"\n",
    "        )  # e.g. cifar10/plane/0001.png\n",
    "        image = np.asarray(Image.open(image_path))  # open image as numpy array\n",
    "        plt_idx = i * num_classes + label + 1  # calculate plot location in the grid\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(image.astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)  # plot class names above columns\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. ImageFolderDataset Implementation\n",
    "\n",
    "Loading images by following the code above is a bit cumbersome. Therefore, the next step is to write a custom **Dataset** class, which takes care of the data loading. This is always the first thing you have to implement when starting a new deep learning project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Dataset Class\n",
    "\n",
    "The **Dataset** class is a wrapper that loads the data from a given file path and returns a python dictionary containing the prepared data, as you have done above. Datasets always need to have the following two methods implemented:\n",
    "- `__len__(self)` is a method that should simply calculate and return the number of images in the dataset. After it is implemented, you can simply call it with `len(dataset)`.\n",
    "- `__getitem__(self, index)` should return the image with the given index from your dataset. Implementing this will allow you to access your dataset like a list, i.e. you can then simply call `dataset[9]` to access the 10th image in the dataset.\n",
    "\n",
    "Generally, you will have to implement a different dataset for every project. However, base dataset classes for future projects will be provided for you in future projects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 ImageFolderDataset Implementation\n",
    "\n",
    "Now it is your turn to implement such a dataset class for CIFAR-10. To do so, open `exercise_code/data/image_folder_dataset.py` and check the following three methods of `ImageFolderDataset`:\n",
    "- `make_dataset(directory, class_to_idx)` should load the prepared data from a given directory root (`directory`) into two lists (`images` and `labels`). `class_to_idx` is a dict mapping class (e.g. 'cat') to label (e.g. 1). **Note**: This function is called in the `__init__` method of the dataset class only ONCE, and is not intended to be called by the user again. \n",
    "- `__len__(self)` should calculate and return the number of images in your dataset.\n",
    "- `__getitem__(self, index)` should return the image with the given index from your dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please read <code>make_dataset(directory, class_to_idx)</code> and make sure to familiarize with its output as you will need to interact with it for the following tasks. Additionally, it would be a wise decision to get familiar with python's os library which will be of utmost importance for most datasets you will write in future projects. As it is not beginner friendly, we removed it for this exercise but it is an important skill for a DL practicer.</p>\n",
    "</div>\n",
    "\n",
    "**Note**: This function is called in the `__init__` method of the dataset class only ONCE, and is not intended to be called by the user again. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__len__(self)</code> method in <code>exercise_code/data/image_folder_dataset.py</code> and test your implementation by running the following cell.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.image_folder_dataset import ImageFolderDataset\n",
    "\n",
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    ")\n",
    "_ = test_len_dataset(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__getitem__(self, index)</code> method in <code>exercise_code/data/image_folder_dataset.py</code> and test your implementation by running the following cell.\n",
    "    </p>\n",
    "    <p><b>Hint:</b> You may want to reuse parts of the '2. Data Visualization' code above in your implementation of <code>__getitem__()</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.image_folder_dataset import ImageFolderDataset\n",
    "\n",
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    ")\n",
    "\n",
    "_ = test_item_dataset(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3 Dataset Usage\n",
    "\n",
    "Now that you have implemented all required parts of the ImageFolderDataset, using the `__getitem__()` method, you can now access our dataset as conveniently as you would access a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_item = dataset[0]\n",
    "sample_image = sample_item[\"image\"]\n",
    "sample_label = sample_item[\"label\"]\n",
    "\n",
    "print('Sample image shape:', sample_image.shape)\n",
    "print('Sample label:', sample_label)\n",
    "print('Sample image first values:', sample_image[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, the images are represented as uint8 values for each of the three RGB color channels. The data type and scale are fundamental in computer vision. `uint8` stands for **unsigned integer 8-bit**. It is a data type that can represent integer values in the range of 0 to 255 RGB values are typically represented as uint8 because it can represent 256 shades of a color, which is sufficient for most applications. Each color component plane can have a range of values from 0 to 255.\n",
    "\n",
    "As you have implemented both `__len__()` and `__getitem__()`, you can now even iterate over the dataset with a simple for loop! \n",
    "\n",
    "**Note**: If it takes too long, just skip this cell. The purpose of the cell is only to show you that your dataset is iterable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_samples = 0\n",
    "max_num_samples = 100\n",
    "\n",
    "for i, sample in enumerate(tqdm(dataset, total=max_num_samples)):\n",
    "    if i >= max_num_samples:\n",
    "        break\n",
    "    num_samples += 1\n",
    "    \n",
    "print(\"Number of samples:\", num_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.4 The crux of loading from disk\n",
    "If you are using google colab or store your files on a local HDD, executing the above cell takes quite some time. This is problematic, since we want to iterate over the dataset in future exercises. The issue is that we access single files every time we access a dataset element, which are then loaded into the memory.\n",
    "\n",
    "Luckily, the CIFAR10 dataset is small enough to fit into the memory for most systems, since it's total file size is around 1.2GB. \n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Warning</h3>\n",
    "    <p>Loading the whole dataset into memory will not work if you are using a machine with 4GB of RAM or less (depending on your other programs such as memory hungry web browsers). Consider closing some open programs or simply use the local on-demand ImageFolderDataset.</p>\n",
    "    <p>In addition we want to warn you that everytime you execute a cell like \"dataset2 = MemoryImageFolderDataset...\" you are loading a 1.2GB matrix into your memory. If you do this often enough this notebook will crash on every machine. Therefore, we make sure to always use a single variable \"dataset\" which will be overwritten by future cells to avoid straining your memory too much.</p>\n",
    "</div>\n",
    "\n",
    "We created a second CIFAR10 dataset for you where we load all images into the memory instantly instead of loading them on demand at access time, which speeds up the loading speed immensly, especially on google colab. You can check out the code under `exercise_coder/data/image_folder_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Don't execute this cell on low RAM machines\n",
    "from exercise_code.data import MemoryImageFolderDataset\n",
    "### colab only ###\n",
    "# import urllib.request\n",
    "### colab only ###\n",
    "\n",
    "dataset = MemoryImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    force_download=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now easily iterate over the whole dataset, once it is loaded into our memory, which should be way faster than the previous iteration. \n",
    "\n",
    "**Note**: If it takes too long, just skip this cell. The purpose of the cell is only to show you that your dataset is iterable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_samples = 0\n",
    "max_num_samples = 100\n",
    "\n",
    "for i, sample in enumerate(tqdm(dataset, total=max_num_samples)):\n",
    "    if i >= max_num_samples:\n",
    "        break\n",
    "    num_samples += 1\n",
    "    \n",
    "print(\"Number of samples:\", num_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.5 Choose your preferred dataset here\n",
    "If you want to switch to the ImageFolderDataset for the remaining notebook, please change the following cell. The default is MemoryImageFolderDataset for efficiency reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DATASET = ImageFolderDataset\n",
    "DATASET = MemoryImageFolderDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Transforms and Image Preprocessing\n",
    "\n",
    "Before training machine learning models, you often need to pre-process the data. For image datasets, two commonly applied techniques are:\n",
    "1. Normalize all images so that each value is either in [-1, 1] or [0, 1]. By doing so the image are also converted to floating point numbers.\n",
    "2. Compute the mean over all images and subtract this mean from all images in the dataset\n",
    "\n",
    "These transform classes are callables, meaning that you will be able to simply use them as follows:\n",
    "\n",
    "```transform = Transform()```\n",
    "\n",
    "```images_transformed = transform(images)```\n",
    "\n",
    "This will be realized in the pipeline by defining so called transforms. Instead of applying them globally to the input data, you will apply those seperatly to each sample after loading it in the `__getitem__` call of the dataset.\n",
    "\n",
    "However, note that you don't need to create any of them yourselvs, as we're doing that for you."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Modify the <code>__getitem__(self, index)</code> method in <code>exercise_code/data/image_folder_dataset.py</code> such that it applies <code>self.transform</code>, if applicablie - if such a transform function was sent as an argument. With this change you can simply define the transforms during dataset creation and apply those automatically for each <code>__getitem__</code> call. Make sure not to break it though ;).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DATASET(\n",
    "    root=cifar_root,\n",
    ")\n",
    "\n",
    "_ = test_transform_dataset(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Equipped with this change, you can now easily add the two preprocessing techniques above for CIFAR-10. You will do so in the following steps by implementing the classes `RescaleTransform` and `NormalizeTransform` in `exercise_code/data/transforms.py`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1 Rescaling Images using RescaleTransform\n",
    "\n",
    "Let's start by implementing `RescaleTransform`. If you look at the `__init__()` method, you will notice it has four arguments:\n",
    "* **out_range** is the range you wish to rescale your images to. E.g. if you want to scale your images to [-1, 1], you would use `range=(-1, 1)`. By default, they will be scaled to [0, 1].\n",
    "* **in_range** is the value range of the data prior to rescaling. For uint8 images, this will always be (0, 255)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__call__()</code> method of <code>RescaleTransform</code> in <code>exercise_code/data/transforms.py</code> and test your implementation by running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.transforms import RescaleTransform\n",
    "\n",
    "rescale_transform = RescaleTransform()\n",
    "dataset_rescaled = DATASET(\n",
    "    root=cifar_root,\n",
    "    transform=rescale_transform\n",
    ")\n",
    "\n",
    "orig_dataset = DATASET(root=cifar_root)\n",
    "_ = test_rescale_transform(orig_dataset, dataset_rescaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you look at the first image, you should now see that all values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_item = dataset_rescaled[0]\n",
    "sample_label = sample_item[\"label\"]\n",
    "sample_image = sample_item[\"image\"]\n",
    "\n",
    "print(\"Max value:\", np.max(sample_image))\n",
    "print(\"Min value:\", np.min(sample_image))\n",
    "print('Sample rescaled image first values:', sample_image[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2 Normalize Images to Standard Gaussian using NormalizeTransform\n",
    "\n",
    "Let us now move on to the `NormalizeTransform` class. The `NormalizeTransform` class normalizes images channel-wise and its `__init__` method has two arguments:\n",
    "* **mean** is the normalization mean, which will be subtracted from the dataset.\n",
    "* **std** is the normalization standard deviation. By scaling the data with a factor of `1/std` the standard deviation will be normazlied accordingly.\n",
    "\n",
    "Have a look at the code in `exercise_code/data/transforms.py`.\n",
    "\n",
    "The next step is to normalize the CIFAR-10 **images channel-wise** to standard normal. To do so, you need to calculate the **per-channel image mean and standard deviation** first, which you can then provide to `NormalizeTransform` to normalize the data accordingly.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Warning</h3>\n",
    "    <p>The next step loads the whole CIFAR10 dataset into your memory so we can work in it. If the notebook crashes it is most likely because you are running out of memory since the following step allocates 1.2GB of memory footprint. If that is an issue for your system, consider not using the MemoryImageFolderDataset or switch to google colab.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You first have to load all rescaled images\n",
    "rescaled_images = []\n",
    "for sample in tqdm(dataset_rescaled, position=0, leave=True):\n",
    "    rescaled_images.append(sample[\"image\"])\n",
    "rescaled_images = np.array(rescaled_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>compute_image_mean_and_std()</code> method and the <code>__call__()</code> method of <code>NormalizeTransform</code> in <code>exercise_code/data/transforms.py</code>. Compute the rescaled dataset's mean and variance by running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.transforms import compute_image_mean_and_std\n",
    "\n",
    "cifar_mean, cifar_std = compute_image_mean_and_std(rescaled_images)\n",
    "print(\"Mean:\\t\", cifar_mean, \"\\nStd:\\t\", cifar_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To test your implementation, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = test_compute_image_mean_and_std(cifar_mean, cifar_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The rescaled images will be deleted now from your ram as they are no longer needed\n",
    "try:\n",
    "    del rescaled_images\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now you can use the mean and standard deviation you computed to normalize the loaded data. This can be done by simply adding the `NormalizeTransform` to the list of transformations our dataset applies in `__getitem__()`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check out the <code>ComposeTransform</code> in <code>transforms.py</code>. Later on, we will most often use multiple transforms and chain them together. Remember that the order is of importance here!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.image_folder_dataset import ImageFolderDataset\n",
    "from exercise_code.data.transforms import RescaleTransform, NormalizeTransform, ComposeTransform\n",
    "\n",
    "# Set up both transforms using the parameters computed above\n",
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "\n",
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    transform=ComposeTransform([rescale_transform, normalize_transform])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code: Run the following cell to see if your normalization worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change the values of these variables - the test validates only this specific pixel.\n",
    "\n",
    "orig_data = ImageFolderDataset(root=cifar_root)\n",
    "_ = test_normalization_transform(orig_data, dataset, cifar_mean, cifar_std)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can now check out the results of the transformed samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_item = dataset[0]\n",
    "sample_label = sample_item[\"label\"]\n",
    "sample_image = sample_item[\"image\"]\n",
    "\n",
    "print('Sample normalized image shape:', sample_image.shape)\n",
    "print('Sample normalized image first values:', sample_image[0][0]) \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Save your Dataset\n",
    "Now save your dataset and transforms using the following cell. This will save it to a pickle file `models/cifar_dataset.p`. We will use this dataset for the next notebook and this will count for the submission."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Note</h3>\n",
    "    <p>Each time you make changes in `dataset`, you need to rerun the following code to make your changes saved, but <b>this is NOT the file which you should submit</b>. You will find the final file for submission in the second notebook.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"dataset\": dataset,\n",
    "        \"cifar_mean\": cifar_mean,\n",
    "        \"cifar_std\": cifar_std,\n",
    "    },\n",
    "    file_name=\"cifar_dataset.p\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Note</h3>\n",
    "    <p>It can take some time for the pickle file to appear on google colab. In that case, strech your limbs and brew a fresh coffee/tea/water before you continue</p>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Key Takeaways\n",
    "1. Always have a look at your data before you start training any models on it.\n",
    "2. Datasets should be organized in corresponding **Dataset** classes that support `__len__` and `__getitem__` methods, which allow us to call `len(dataset)` and `dataset[index]`.\n",
    "3. Data often needs to be preprocessed. Such preprocessing can be implemented in **Transform** classes, which are callables that can be simply applied via `data_transformed = transform(data)`. However, we will rarely do that and apply transforms on the fly using a dataloader which we will introduce in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9fd0696167aaed30d55c31fd713b4c23f5cf987c5457682b34d44d5309ecf99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
