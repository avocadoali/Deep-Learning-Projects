{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Dataset: Preprocessing\n",
    "\n",
    "In this week's exercise, we want to train a simple classifier. The underlying dataset is a dataset that contains approximately 1400 samples. Each sample is representing a house and the dataset provides 81 features.\n",
    "\n",
    "This notebook will demonstrate the preprocessing pipeline that we use to bring the data in a form so that we can use it for the training of our simple classifier. This notebook is optional and there will be no tasks for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us first import some libraries and code that we will need along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment thefollowing cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_04) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_04'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dataloading\n",
    "\n",
    "Before we start preprocessing our data, let us first download the dataset and use the ```CSVDataset``` class to access the downloaded dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "housing_file_path = os.path.join(root_path, \"housing_train.csv\")\n",
    "download_url = 'https://i2dl.vc.in.tum.de/static/data/housing_train.zip'\n",
    "\n",
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "target_column = 'SalePrice'\n",
    "train_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see the dataset in ```i2dl_exercises/datasets/housing``` in your file browser, which should contain a csv file containing all the data. \n",
    "\n",
    "It is always a good idea to get an overview of how our dataset looks like. By executing the following cell you can see some data samples. For each house, our dataset provides 81 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 80 features of our models (apart from the target). But not all the features are correlated with our target 'SalePrice'. So we need to perform a feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.df.corr()[target_column].sort_values(ascending=False)[:3]\n",
    "numeric_cols = train_dataset.df.select_dtypes(include=np.number).columns\n",
    "corr_matrix = train_dataset.df[numeric_cols].corr()\n",
    "corr_values = corr_matrix[target_column].sort_values(ascending=False)[:3]\n",
    "train_dataset.df[numeric_cols] = train_dataset.df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our classifier is a very simple version we restrict our model to only one of the given features. In our case, let us select the feature ```GrLivArea``` and use this one to predict the target column , which will be the feature ```SalePrice```. This setting has the advantage that we can easily visualize our data in a 2 dimensional setting. Of course, a greater choice of features would make our model more powerful and accurate. But as we said, we want to keep it simple here and focus on the training process. The required data for training our model will then reduce to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected feature and target \n",
    "train_dataset.df[['GrLivArea',target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a scatter plot, we can visualize the relationship between â€˜GrLivAreaâ€™ and 'SalePrice'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_dataset.df[['GrLivArea']], train_dataset.df[[target_column]])\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "The features are at very different scales and variances. Therefore, we normalize the features ranges with the minimum and maximum value of each numeric column. For filling in missing numeric values (if any), we need the mean value. These values should be precomputed on the training set and used for all dataset splits. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>The <code>FeatureSelectorAndNormalizationTransform</code> class defined in <code>exercise_code/data/csv_dataset.py</code> is implementing this transformation. Make sure you have a look at the code of this file to understand the next cells. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_dataset.df\n",
    "# Select a feature to keep plus the target column.\n",
    "selected_columns = ['GrLivArea', target_column]\n",
    "mn, mx, mean = \\\n",
    "        df.min(skipna=True, numeric_only=True), df.max(skipna=True, numeric_only=True),\\\n",
    "            df.mean(skipna=True, numeric_only=True)\n",
    "\n",
    "\n",
    "column_stats = {}\n",
    "for column in selected_columns:\n",
    "    crt_col_stats = {'min' : mn[column],\n",
    "                     'max' : mx[column],\n",
    "                     'mean': mean[column]}\n",
    "    column_stats[column] = crt_col_stats    \n",
    "\n",
    "transform = FeatureSelectorAndNormalizationTransform(column_stats, target_column)\n",
    "\n",
    "def rescale(data, key = \"SalePrice\", column_stats = column_stats):\n",
    "    \"\"\" Rescales input series y\"\"\"\n",
    "    mx = column_stats[key][\"max\"]\n",
    "    mn = column_stats[key][\"min\"]\n",
    "\n",
    "    return data * (mx - mn) + mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having computed the ```min```, ```max``` and ```mean``` value, we load the data splits and perform the transformation on our data using the ```CSVDataset``` class. To check whether the partitions are correct, we print for each one of them the number of samples. Remember to not touch the test set until you are done with the training of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "train_dataset = CSVDataset(mode=\"train\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "val_dataset = CSVDataset(mode=\"val\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "test_dataset = CSVDataset(mode=\"test\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the respective data splits ('train', 'val, and 'test') into one matrix of shape $N \\times D$ where $N$ represents the number of samples and $D$ the number of features (in our case we only have one feature). Similarly, we load the target data in one matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data into a matrix of shape (N, D), same for targets resulting in the shape (N, 1)\n",
    "X_train = [train_dataset[i]['features'] for i in range((len(train_dataset)))]\n",
    "X_train = np.stack(X_train, axis=0)\n",
    "y_train = [train_dataset[i]['target'] for i in range((len(train_dataset)))]\n",
    "y_train = np.stack(y_train, axis=0)\n",
    "print(\"train data shape:\", X_train.shape)\n",
    "print(\"train targets shape:\", y_train.shape)\n",
    "\n",
    "# load validation data\n",
    "X_val = [val_dataset[i]['features'] for i in range((len(val_dataset)))]\n",
    "X_val = np.stack(X_val, axis=0)\n",
    "y_val = [val_dataset[i]['target'] for i in range((len(val_dataset)))]\n",
    "y_val = np.stack(y_val, axis=0)\n",
    "print(\"val data shape:\", X_val.shape)\n",
    "print(\"val targets shape:\", y_val.shape)\n",
    "\n",
    "# load test data\n",
    "X_test = [test_dataset[i]['features'] for i in range((len(test_dataset)))]\n",
    "X_test = np.stack(X_test, axis=0)\n",
    "y_test = [test_dataset[i]['target'] for i in range((len(test_dataset)))]\n",
    "y_test = np.stack(y_test, axis=0)\n",
    "print(\"test data shape:\", X_test.shape)\n",
    "print(\"test targets shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "# 0 encodes small prices, 1 encodes large prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we model our binary classification problem. We divide our target in the categories ```low-priced``` and ```expensive``` by labeling the 30% of the houses that are sold with the lowest price with ```0``` and, accordingly, the 30% of the houses with the highest price with ```1```. All other houses will be deleted from our data. We will use the  method ```binarize()```. For more information, take a look at the file ```networks/utils.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.utils import binarize\n",
    "y_all = np.concatenate([y_train, y_val, y_test])\n",
    "thirty_percentile = np.percentile(y_all, 30)\n",
    "seventy_percentile = np.percentile(y_all, 70)\n",
    "\n",
    "# Prepare the labels for classification.\n",
    "X_train, y_train = binarize(X_train, y_train, thirty_percentile, seventy_percentile )\n",
    "X_val, y_val   = binarize(X_val, y_val, thirty_percentile, seventy_percentile)\n",
    "X_test, y_test  = binarize(X_test, y_test, thirty_percentile, seventy_percentile)\n",
    "\n",
    "print(\"train data shape:\", X_train.shape)\n",
    "print(\"train targets shape:\", y_train.shape)\n",
    "print(\"val data shape:\", X_val.shape)\n",
    "print(\"val targets shape:\", y_val.shape)\n",
    "print(\"test data shape:\", X_test.shape)\n",
    "print(\"test targets shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we reduced our data and the remaining houses in our dataset are now either labeled with ```1``` and hence categorized as ```expensive```, or they are labeled with ```0``` and hence categorized as ```low-priced```.\n",
    "\n",
    "The data is now ready and can be used to train our classifier model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "430b257a676bba6f7ef2b8c624438ed51b2404cb50b5575e36e7fcc7dfc10f81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
