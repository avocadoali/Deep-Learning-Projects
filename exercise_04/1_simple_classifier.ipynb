{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classifier / Logistic Regression\n",
    "\n",
    "After having worked with the dataloading part last week, we want to start this week with a more detailed look into how the training process works. So far, our tools are limited and we must restrict ourselves to a simplified model. But nevertheless, this gives us the opportunity to look at the different parts of the training process in more detail and builds up a good base for when we turn to more complicated model architectures in the next exercises.\n",
    "\n",
    "This notebook will demonstrate a simple logistic regression model predicting whaether a house is ```low-priced``` or ```expensive```. The data that we will use here is the HousingPrice dataset. Feeding some features in our classifier, the output should then be a score that determines in which category the considered house is.\n",
    "\n",
    "<img name=\"classifierTeaser\" src=\"https://drive.google.com/uc?id=1QX7hqHIjq0FF-bTfoZc_y9gZptdsm2nm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us first import some libraries and code that we will need along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment thefollowing cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\nimport os\\n\\ngdrive_path='/content/gdrive/MyDrive/i2dl/exercise_04'\\n\\n# This will mount your google drive under 'MyDrive'\\ndrive.mount('/content/gdrive', force_remount=True)\\n# In order to access the files in this notebook we have to navigate to the correct folder\\nos.chdir(gdrive_path)\\n# Check manually if all files are present\\nprint(sorted(os.listdir()))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE 1: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_04) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_04'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dataloading and Data Preprocessing\n",
    "\n",
    "Let us load the data that we want to use for our training. The method `get_housing_data()` is providing you with a training, validation and test set that is ready to use.\n",
    "\n",
    "For more information about how to prepare the data and what the final data look like, you can have a look at the notebook `housing_data_preprocessing(optional).ipynb `. We reduced our data and the remaining houses in our dataset are now either labeled with ```1``` and hence categorized as ```expensive```, or they are labeled with ```0``` and hence categorized as ```low-priced```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/.conda/envs/i2dl/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/ali/.conda/envs/i2dl/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/ali/.conda/envs/i2dl/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/home/ali/.conda/envs/i2dl/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You successfully loaded your data! \n",
      "\n",
      "train data shape: (533, 1)\n",
      "train targets shape: (533, 1)\n",
      "val data shape: (167, 1)\n",
      "val targets shape: (167, 1)\n",
      "test data shape: (177, 1)\n",
      "test targets shape: (177, 1) \n",
      "\n",
      "The original dataset looks as follows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>530</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32668</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Alloca</td>\n",
       "      <td>200624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>492</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>79.0</td>\n",
       "      <td>9490</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>460</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7015</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Bnk</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>280</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>83.0</td>\n",
       "      <td>10005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>656</td>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1680</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Family</td>\n",
       "      <td>88000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "529  530          20       RL          NaN    32668   Pave   NaN      IR1   \n",
       "491  492          50       RL         79.0     9490   Pave   NaN      Reg   \n",
       "459  460          50       RL          NaN     7015   Pave   NaN      IR1   \n",
       "279  280          60       RL         83.0    10005   Pave   NaN      Reg   \n",
       "655  656         160       RM         21.0     1680   Pave   NaN      Reg   \n",
       "\n",
       "    LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "529         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "491         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "459         Bnk    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "279         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "655         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "    MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "529      3   2007        WD         Alloca     200624  \n",
       "491      8   2006        WD         Normal     133000  \n",
       "459      7   2009        WD         Normal     110000  \n",
       "279      3   2008        WD         Normal     192000  \n",
       "655      3   2010        WD         Family      88000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exercise_code.networks.utils import *\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, train_dataset = get_housing_data()\n",
    "\n",
    "print(\"train data shape:\", X_train.shape)\n",
    "print(\"train targets shape:\", y_train.shape)\n",
    "print(\"val data shape:\", X_val.shape)\n",
    "print(\"val targets shape:\", y_val.shape)\n",
    "print(\"test data shape:\", X_test.shape)\n",
    "print(\"test targets shape:\", y_test.shape, '\\n')\n",
    "\n",
    "print('The original dataset looks as follows:')\n",
    "train_dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready and can be used to train our classifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up a Classifier Model\n",
    "\n",
    "Let $\\mathbf{X} \\in \\mathbb{R}^{N\\times (D+1)}$ be our data with $N$ samples and $D$ feature dimensions (+1 for the bias). With our classifier model, we want to predict binary labels $\\mathbf{\\hat{y}} \\in \\mathbb{R}^{N\\times 1}$. Our classifier model should be of the form\n",
    "\n",
    "$$ \\mathbf{\\hat{y}}  = \\sigma \\left( \\mathbf{X} \\cdot \\mathbf{w} \\right), $$ \n",
    "\n",
    "$ $ where $\\mathbf{w}\\in \\mathbb{R}^{(D+1) \\times 1}$ is the weight matrix of our model.\n",
    "\n",
    "The **sigmoid function** $\\sigma: \\mathbb{R} \\to [0, 1]$, defined by \n",
    "\n",
    "$$ \\sigma(t) = \\frac{1}{1+e^{-t}} $$\n",
    "\n",
    "is used to squash the outputs of the linear layer into the interval $[0, 1]$. The layer is saturated, when the output approaches its upper or lower boundaries. Remember that the sigmoid function is a real-valued function. When applying it on a vector, the sigmoid is operating component-wise.\n",
    "\n",
    "The output of the sigmoid function can be seen as the probability that our sample is indicating a house that can be categorized as ```expensive```. As the probability gets closer to 1, our model is more confident that the input sample is in the class ```expensive```.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Take a look at the implementation of the <code>Classifier</code> class in <code>exercise_code/networks/classifier.py</code>. To create a <code>Classifier</code> object, you need to define the number of features that our classifier model takes as input.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss: Binary Cross Entropy\n",
    "\n",
    "For a binary classification like our task, we use a loss function called Binary Cross-Entropy (BCE).\n",
    "\n",
    "$$BCE(y,\\hat{y}) =- \\frac{1}{N} \\sum_{i = 1}^N \\left[y_i \\cdot log(\\hat y_i ) + (1- y_i) \\cdot log(1-\\hat y_i)\\right]$$\n",
    "\n",
    "where $y\\in\\mathbb{R}$ is the ground truths vector, $\\hat y\\in\\mathbb{R}$ is the vector of predicted probabilities of the houses being expensive and $N$ is the number of samples.\n",
    "\n",
    "In constrast to linear-regression, for the BCE there is no closed-form solution for the optimal weights vector. In order to find the optimal parameters for our model, we need to use numeric methods such as **Gradient Descent**. But let us have a look at that later. First, you have to complete your first task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>In <code>exercise_code/networks/loss.py</code> complete the implementation of the BCE loss function. You need to write the forward and backward pass of BCE as <code>forward()</code> and <code>backward()</code> function. The backward pass of the loss is needed to later optimize your weights of the model. You can test your implementation by executing the included testing code in the cell below.</p>\n",
    "    <p>Note: Both functions must be implemented for the backward test to pass! </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing \u001b[96mBCETest\u001b[0m Started #######\n",
      "\n",
      "Test BCEForwardTest: \u001b[92mpassed!\u001b[0m\n",
      "Test BCEBackwardTestNormal: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing \u001b[96mBCETest\u001b[0m Finished #######\n",
      "Test BCETest: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m2\u001b[0m/\u001b[92m2\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.tests.loss_tests import *\n",
    "from exercise_code.networks.loss import BCE\n",
    "\n",
    "bce_loss = BCE()\n",
    "res = test_bce(bce_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation\n",
    "\n",
    "The backpropagation algorithm allows the information from the loss flowing backward through the network in order to compute the gradient of the loss function $L$ w.r.t the weights $w$ of the model and the layers' inputs $x$. \n",
    "\n",
    "The key idea of backpropagation is decomposing the derivatives by applying the chain rule to the loss function.\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial w}$$\n",
    "\n",
    "You have already completed the `forward()` and `backward()` pass of the loss function, which can be used to compute the derivative  $\\frac{\\partial L}{\\partial \\hat y}$. In order to compute the second term $\\frac{\\partial \\hat y}{\\partial w}$, we need to implement a similar `forward()` and `backward()` method in our `Classifier` class.\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "The backward pass consists of computing the derivative $\\frac{\\partial \\hat y}{\\partial w}$. Again, we can decompose this derivative by the chain rule: For $s = X \\cdot w$ we obtain\n",
    "\n",
    "$$\\frac{\\partial \\hat y}{\\partial w} = \\frac{\\partial \\sigma(s)}{\\partial w} = \\frac{\\partial \\sigma(s)}{\\partial s} \\cdot \\frac{\\partial s}{\\partial w}$$\n",
    "\n",
    "\n",
    "**Hint:** Taking track of the dimensions in higher-dimensional settings can make the task a little bit complicated. Make sure you understand the operations here. If you have difficulties, then first try to understand the forward and backward pass with a single input consisting of $D+1$ features. In that case our data matrix has the dimension $X \\in \\mathbb{R}^{1 \\times (D+1)}$. After you have understood this situation, you can go back to the setting where our data matrix has dimension $X \\in \\mathbb{R}^{N \\times (D+1)}$ and consists of $N$ samples each having $D+1$ features.\n",
    "\n",
    "**Hint 2**: It is helpful to follow the [TUM article][] (Section 3) calculating the chain-rule, while dealing with matrix notations:\n",
    "\n",
    "**Note**: If $X$ is of shape $N\\times D$, then in this exercise it is $N\\times (D+1)$, as we concatanate the affine layer's bias term to it, instead of having a different variable.\n",
    "\n",
    "[TUM article]: https://bit.ly/tum-article \"Article\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>forward()</code> and <code>backward()</code> pass as well as the <code>sigmoid()</code> function in the <code>Classifier</code> class in <code>exercise_code/networks/classifier.py</code>. Check your implementation using the following testing code.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing \u001b[96mClassifierTest\u001b[0m Started #######\n",
      "\n",
      "\n",
      "####### Testing \u001b[96mSigmoidMethodTest\u001b[0m Started #######\n",
      "\n",
      "Test Sigmoid_Of_Zero: \u001b[92mpassed!\u001b[0m\n",
      "Test Sigmoid_Of_Zero_Array: \u001b[92mpassed!\u001b[0m\n",
      "Test Sigmoid_Of_100: \u001b[92mpassed!\u001b[0m\n",
      "Test Sigmoid_Of_Array_of_100: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing \u001b[96mSigmoidMethodTest\u001b[0m Finished #######\n",
      "Method sigmoid(): \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m4\u001b[0m/\u001b[92m4\u001b[0m\n",
      "\n",
      "####### Testing \u001b[96mForwardMethodTest\u001b[0m Started #######\n",
      "\n",
      "Test ClassifierForwardTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing \u001b[96mForwardMethodTest\u001b[0m Finished #######\n",
      "Method forward(): \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m1\u001b[0m/\u001b[92m1\u001b[0m\n",
      "\n",
      "####### Testing \u001b[96mBackwardMethodTest\u001b[0m Started #######\n",
      "\n",
      "Test ClassifierBackwardTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing \u001b[96mBackwardMethodTest\u001b[0m Finished #######\n",
      "Method backward(): \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m1\u001b[0m/\u001b[92m1\u001b[0m\n",
      "\n",
      "####### Testing \u001b[96mClassifierTest\u001b[0m Finished #######\n",
      "Test ClassifierTest: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m6\u001b[0m/\u001b[92m6\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.networks.classifier import Classifier\n",
    "from exercise_code.tests.classifier_test import *\n",
    "res = test_classifier(Classifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizer and Gradient Descent\n",
    "\n",
    "Previously, we have successfully dealt with the loss function, which is a method of measuring how well our model fits the given data. The idea of the training process is to iteratively adjust the weights of our model in order to minimize the loss function.\n",
    "\n",
    "And this is where the optimizer comes steps in. During each training step, the optimizer updates the weights of the model w.r.t. the output of the loss function, thereby linking the loss function and model parameters together. The goal is to obtain a model which is accurately predicting the class for a new sample.\n",
    "\n",
    "\n",
    "Any discussion about optimizers needs to begin with the most popular one, and it's called Gradient Descent. This algorithm is used across all types of Machine Learning (and other math problems) to optimize. It's fast, robust, and flexible. Here's how it works:\n",
    "\n",
    "\n",
    "0. Initialize the weights with random values.\n",
    "1. Run the forward pass and calculate the loss with the current weights and the loss function.\n",
    "2. Calculate the gradient of the loss function w.r.t. the weights.\n",
    "3. Update weights with the corresponding gradient.\n",
    "4. Iteratively perform Step 1 to 3 until converges.\n",
    "\n",
    "The name of the optimizer already hints at the required concept: We use gradients which are very useful for minimizing a function. The gradient of the loss function w.r.t to the weights $w$ of our model tells us how to change our weights $w$ in order to minimize our loss function. \n",
    "\n",
    "The weights are updated each step as follows:\n",
    "$$ w_{(n+1)} = w_{(n)} - \\alpha \\cdot \\frac {dL}{dw_{(n)}}, $$\n",
    "where $ \\frac {dL}{dw_{(n)}}$ is the gradient of your loss function w.r.t. the weights $w$ at the $n$-th optimization step and $\\alpha$ is the learning rate, which is a predefined positive scalar (usually $ 0 < \\alpha < 1 $) determining the size of the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>In our model, we will use gradient descent to update the weights. Take a look at the <code>Optimizer</code> class in the file <code>networks/optimizer.py</code>. Your task is now to implement the gradient descent step in the <code>step()</code> method. You can test your implementation by the following testing code.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import Optimizer\n",
    "from exercise_code.networks.classifier import Classifier\n",
    "from exercise_code.tests.optimizer_test import *\n",
    "TestClassifier=Classifier()\n",
    "TestClassifier.initialize_weights()\n",
    "res = test_optimizer(Optimizer(TestClassifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "We have now implemented all the necessary parts of our training process, namely:\n",
    "- **Classifier Model:** We set up a simple classifier model and you implemented the corresponding ```forward()``` and ```backward()``` methods.\n",
    "- **Loss function:** We chose the Binary Cross Entropy Loss for our model to measure the distance between the prediction of our model and the ground-truth labels. You implemented a forward and backward pass for the loss function.\n",
    "- **Optimizer**: We use the Gradient Descent method to update the weights of our model. Here, you implemented the ```step()``` function which performs the update of the weights. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Before we start our training and put all the parts together, let us shortly talk about the weight initialization. In <code>networks/classifier.py</code> you can check the <code>Classifier</code> class. It contains a method called <code>initialize_weights()</code> that randomly initializes the weights of our classifier model. Later in the lecture, we will learn about more efficient methods to initialize the weights. But for now, a random initialization as it happens in the <code>initialize_weights()</code> method is sufficient.</p>\n",
    "</div>\n",
    "\n",
    "Let's start with our classifier model and look at its performance before any training happened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.classifier import Classifier\n",
    "\n",
    "#initialization\n",
    "model = Classifier(num_features=1)\n",
    "model.initialize_weights()\n",
    "\n",
    "y_out = model.forward(X_train)\n",
    "\n",
    "# plot the prediction\n",
    "plt.scatter(X_train, y_train)\n",
    "plot = plt.plot(X_train, y_out, color='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the predictions of our model without any training are very bad. Let's see how the performance improves when we start our training, which means that we update our weights by applying the gradient descent method. The following cell combines the forward and backward passes with the gradient update step and performs a training step for our classifier:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Note that the <code>Classifier</code> class is derived from the more general <code>Network</code> class. It is worth having a look at the basis class <code>Network</code> in the file <code>exercise_code/networks/base_networks.py</code>. We will make use of the <code>__call__()</code> method, which computes the forward and backward pass of your classifier. In a similar manner, we use the <code>__call__()</code> function for our Loss function.</p>\n",
    "</div>\n",
    "\n",
    "The following cell performs training with 400 training steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import *\n",
    "from exercise_code.networks.classifier import *\n",
    "from exercise_code.tests.base_tests import bcolors\n",
    "\n",
    "# Hyperparameter Setting. We will specify the loss function we use, and implement the optimizer we finished in the last step.\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# initialization\n",
    "model = Classifier(num_features=num_features)\n",
    "model.initialize_weights()\n",
    "\n",
    "loss_func = BCE() \n",
    "learning_rate = 5e-1  # A hyperparameter\n",
    "loss_history = []\n",
    "optimizer = Optimizer(model, learning_rate)\n",
    "\n",
    "epochs = 400 # A hyperparameter\n",
    "print_every = 10 # A hyperparameter\n",
    "\n",
    "best_loss = np.inf\n",
    "# Full batch Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Enable your model to store the gradient.\n",
    "    model.train()\n",
    "    # Compute the output and gradients w.r.t weights of your model for the input dataset.\n",
    "    model_forward = model.forward(X_train)\n",
    "    \n",
    "    # Compute the loss and gradients w.r.t output of the model. The begining of the chain rule.\n",
    "    loss = loss_func(model_forward, y_train)\n",
    "    loss_grad = loss_func.backward(model_forward, y_train)\n",
    "\n",
    "    # Send the upstream derivative to continue the chain rule.\n",
    "    grad = model.backward(loss_grad)\n",
    "    \n",
    "    optimizer.step(grad)\n",
    "    \n",
    "    # Average over the loss of the entire dataset and store it.\n",
    "    average_loss = np.mean(loss)\n",
    "    loss_history.append(average_loss)\n",
    "    if i % print_every == 0:\n",
    "        if average_loss < best_loss:\n",
    "            best_loss = average_loss\n",
    "            average_loss = bcolors.colorize(\"green\", average_loss)\n",
    "        else:\n",
    "            average_loss = bcolors.colorize(\"red\", average_loss)\n",
    "        print(\"Epoch \",i,\"--- Average Loss: \", average_loss)\n",
    "\n",
    "model.eval() # Change the node of the network to evaluation mode!\n",
    "model_forward = model(X_test)\n",
    "\n",
    "accuracy = test_accuracy(model_forward, y_test)\n",
    "print(\"\\nEvaluate the trained model on the X_test set: \")\n",
    "print(\"Accuracy: {:.1f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our average loss is decreasing as expected. Let us visualize the average loss and the prediction after our short training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history to see how it goes after several steps of gradient descent.\n",
    "plt.plot(loss_history, label = 'Train Loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# forward pass\n",
    "y_out = model(X_test)\n",
    "\n",
    "# plot the prediction\n",
    "plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "inds = X_test.argsort(0).flatten()\n",
    "plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "plt.legend()\n",
    "plt.title('Prediction of your trained model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good already and our model gets better in explaining the underlying relationship of data.\n",
    "\n",
    "NOTE: Although the testing score is high, the above prediction graph is still somewhat of a poor performance. That is due to the threshold (Look at the implementation of the test_accuracy() function), which classifies the results, or \"logits\", to the binary classes. If the threshold value is $t \\in \\mathbb{R}$, then for $1 \\leq i \\leq N$, we have  \n",
    "$$ g(x_i) = \\begin{cases}\n",
    "    1,& \\text{if }\\, \\hat y_i > t \\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $g(x)$ is the classifier function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Solver\n",
    "\n",
    "Now we want to put everything we have learned so far together in an organized and concise way, that provides easy access to train a network/model in your own script/code. The purpose of a solver is mainly to provide an abstraction for all the gritty details behind training your parameters, such as logging your progress, optimizing your model, and handling your data.\n",
    "\n",
    "This part of the exercise will require you to complete the missing code in the ```Solver``` class and to train your model end to end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Open the file <code>exercise_code/solver.py</code> and have a look at the <code>Solver</code> class. The <code>_step()</code> function is representing one single training step. So when using the Gradient Descent method, it represents one single update step using the Gradient Descent method. Your task is now to finalize this <code>_step()</code> function. You can test your implementation with the testing code included in the following cell.</p>\n",
    "    <p> <b>Hint</b>: The implementation of the <code>_step()</code> function is very similar to the implementation of a training step as we observed above. You may have a look at that part first. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.classifier import Classifier\n",
    "from exercise_code.tests.solver_tests import *\n",
    "weights = np.array([[0.1],[0.1]])\n",
    "TestClassifier = Classifier(num_features=1)\n",
    "TestClassifier.initialize_weights(weights)\n",
    "learning_rate = 5e-1\n",
    "data = {'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val}\n",
    "loss = BCE()\n",
    "solver = Solver(TestClassifier, data, loss,learning_rate,verbose=True)\n",
    "\n",
    "res = test_solver(solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having successfully implemented the `step()` function in the `Optimizer` class, let us now train our classifier. We train our model with a learning rate $ \\lambda = 0.1$ and with 25000 epochs. Your model should reach an accuracy which is higher than 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.utils import test_accuracy\n",
    "from exercise_code.networks.classifier import Classifier\n",
    "# Select the number of features, you want your task to train on.\n",
    "num_features = X_train.shape[1]\n",
    "print(\"Number of features: \", num_features)\n",
    "\n",
    "# initialize model and weights\n",
    "model = Classifier(num_features=num_features)\n",
    "model.initialize_weights()\n",
    "\n",
    "y_out = model(X_test)\n",
    "\n",
    "accuracy = test_accuracy(y_out, y_test)\n",
    "print(\"Accuracy BEFORE training {:.1f}%\".format(accuracy*100))\n",
    "\n",
    "\n",
    "if np.shape(X_test)[1]==1:\n",
    "    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "    inds = X_test.flatten().argsort(0)\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "data = {'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val}\n",
    "\n",
    "#We use the BCE loss\n",
    "loss = BCE()\n",
    "\n",
    "# Please use these hyperparameters as we also use them later in the evaluation\n",
    "learning_rate = 1e-1\n",
    "epochs = 25000\n",
    "\n",
    "# Setup for the actual solver that's going to do the job of training\n",
    "# the model on the given data. set 'verbose=True' to see real time \n",
    "# progress of the training. \n",
    "#\n",
    "# Note: Too many epochs will result in OVERFITTING - the training loss\n",
    "# will shrink towards zero, while the performance on the test set is actually worsened.\n",
    "\n",
    "solver = Solver(model, \n",
    "                data, \n",
    "                loss,\n",
    "                learning_rate, \n",
    "                verbose=True, \n",
    "                print_every = 1000)\n",
    "\n",
    "# Train the model, and look at the results.\n",
    "solver.train(epochs)\n",
    "\n",
    "\n",
    "# Test final performance\n",
    "y_out = model(X_test)\n",
    "accuracy = test_accuracy(y_out, y_test)\n",
    "print(\"Accuracy AFTER training {:.1f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process losses in each epoch are stored in the lists `solver.train_loss_history` and `solver.val_loss_history`. We can use them to plot the training result easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(solver.val_loss_history, label = \"Validation Loss\")\n",
    "plt.plot(solver.train_loss_history, label = \"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend() \n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show() \n",
    "\n",
    "\n",
    "if np.shape(X_test)[1]==1:\n",
    "\n",
    "    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "    inds = X_test.argsort(0).flatten()\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.title('Prediction of your trained model')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save your BCE Loss, Classifier and Solver for Submission\n",
    "\n",
    "Your model should be trained now and able to predict whether a house is expensive or not. Hooooooray, you trained your very first model! The model will be saved as a pickle file to `models/simple_classifier.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "from exercise_code.networks.loss import BCE\n",
    "from exercise_code.networks.classifier import Classifier\n",
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.optimizer import Optimizer\n",
    "\n",
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"BCE_class\": BCE,\n",
    "        \"Classifier_class\": Classifier,\n",
    "        \"Optimizer\": Optimizer,\n",
    "        \"Solver_class\": Solver\n",
    "    },\n",
    "    file_name=\"simple_classifier.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Now, that you have completed the necessary parts in the notebook, you can go on and submit your files.\n",
    "\n",
    "1. Go on [our submission page](https://i2dl.vc.in.tum.de/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum-online and check your mails there. You will get an id which we need in the next step.\n",
    "2. Log into [our submission page](https://i2dl.vc.in.tum.de/) with your account details and upload the zip file.\n",
    "3. Your submission will be evaluated by our system and you will get feedback about the performance of it. You will get an email with your score as well as a message if you have surpassed the threshold.\n",
    "4. Within the working period, you can submit as many solutions as you want to get the best possible score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('../output/exercise_04')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Goals\n",
    "\n",
    "For this exercise we only test your implementations which are tested throughout the notebook.  In total we have 10 test cases, where you are required to complete at least 8. Here is an overview split among the notebook:\n",
    "\n",
    "- Goal: \n",
    "    - To implement: \n",
    "        1. `exercise_code/networks/loss.py`: `forward()`, `backward()`\n",
    "        2. `exercise_code/networks/classifier.py`: `forward()`, `backward()`, `sigmoid()`\n",
    "        3. `exercise_code/networks/optimizer.py`: `step()`\n",
    "        4. `exercise_code/solver.py`: `_step()`\n",
    "\n",
    "    - Test cases:\n",
    "      1. Does `forward()` of `BCE` return the correct value?\n",
    "      2. Does `backward()` of `BCE` return the correct value?\n",
    "      3. Does `sigmoid()` of `Classifier` return the correct value when `x=0`?\n",
    "      4. Does `sigmoid()` of `Classifier` return the correct value when `x=np.array([0,0,0,0,0])`?\n",
    "      5. Does `sigmoid()` of `Classifier` return the correct value when `x=100`?\n",
    "      6. Does `sigmoid()` of `Classifier` return the correct value when `x=np.asarray([100, 100, 100, 100, 100])`?\n",
    "      7. Does `forward()` of `Classifier` return the correct value?\n",
    "      8. Does `backward()` of `Classifier` return the correct value?\n",
    "      9. Does `Optimizer` update the model parameter correctly?\n",
    "      10. Does `Solver` update the model parameter correctly?\n",
    "    \n",
    "<br />\n",
    "\n",
    "- Reachable points [0, 100]: 0 if not implemented, 100 if all tests passed, 10 per passed test\n",
    "- Threshold to pass the exercise: 80\n",
    "- You can make multiple submissions until the deadline. Your __best submission__ will be considered for bonus.\n",
    "- Submission webpage: https://i2dl.vc.in.tum.de/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "54970da6898dad277dbf355945c2dee7f942d2a31ec1fc1455b6d4f552d07b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
