{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Segmentation \n",
    "============\n",
    "\n",
    "In this task, you will be focusing on semantic segmentation, a computer vision technique. Unlike image classification, the objective here is not to classify the entire image as a whole, but rather to classify each individual pixel within the image. Consequently, the network's output is not a single value but a segmentation map with the same dimensions as the input image. Think about why convolutional layers are preferable over fully-connected layers for this particular task.\n",
    "\n",
    "\n",
    "<img src='https://i2dl.vc.in.tum.de/static/images/exercise_10//segmentation.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_10) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_10'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up PyTorch environment in colab\n",
    "- (OPTIONAL) Enable GPU via Runtime --> Change runtime type --> GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from exercise_code.data.segmentation_dataset import SegmentationData, label_img_to_rgb\n",
    "from exercise_code.data.download_utils import download_dataset\n",
    "from exercise_code.util import visualizer, save_model\n",
    "from exercise_code.util.Util import checkSize, checkParams, test\n",
    "from exercise_code.networks.segmentation_nn import SegmentationNN, DummySegmentationModel\n",
    "from exercise_code.tests import test_seg_nn\n",
    "\n",
    "#set up default cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorBoard\n",
    "In exercise 07 you've already learned how to use TensorBoard. Let's use it again to make the debugging of our network and training process more convenient! As you progress through this notebook, don't hesitate to incorporate additional logs or visualizations in TensorBoard to further enhance your understanding and monitoring of the network's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('logs/introduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./ --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Visualize Data\n",
    "\n",
    "#### MSRC-v2 Segmentation Dataset\n",
    "\n",
    "The MSRC-v2 dataset is an expanded version of the MSRC-v1 dataset, originally created by Microsoft Research in Cambridge. This dataset consists of 591 images, each annotated with accurate pixel-level labels. It contains 23 distinct object classes like bike, bird, car, house or tree.\n",
    "\n",
    "\n",
    "The image ids are stored in the txt file `train.txt`, `val.txt`, `test.txt`. The dataloader will read the image id in the txt file and fetch the corresponding input and target images from the image folder. \n",
    "<img src='https://i2dl.vc.in.tum.de/static/images/exercise_10/input_target.png'/>\n",
    "\n",
    "\n",
    "\n",
    "As you can see in `exercise_code/data/segmentation_dataset.py`, each segmentation label has its corresponding RGB value stored in the `SEG_LABELS_LIST`. The label `void` means `unlabeled`, and it is displayed as black `\"rgb_values\": [0, 0, 0]` in the target image. The target image pixels will be labeled based on its color using `SEG_LABELS_LIST`.\n",
    "\n",
    "```python\n",
    "                SEG_LABELS_LIST = [\n",
    "                {\"id\": -1, \"name\": \"void\",       \"rgb_values\": [0,   0,    0]},\n",
    "                {\"id\": 0,  \"name\": \"building\",   \"rgb_values\": [128, 0,    0]},\n",
    "                {\"id\": 1,  \"name\": \"grass\",      \"rgb_values\": [0,   128,  0]},\n",
    "                {\"id\": 2,  \"name\": \"tree\",       \"rgb_values\": [128, 128,  0]},\n",
    "                {\"id\": 3,  \"name\": \"cow\",        \"rgb_values\": [0,   0,    128]},\n",
    "                {\"id\": 4,  \"name\": \"horse\",      \"rgb_values\": [128, 0,    128]},\n",
    "                {\"id\": 5,  \"name\": \"sheep\",      \"rgb_values\": [0,   128,  128]},\n",
    "                ...]    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <h3>Note: The label <code>void</code></h3>\n",
    "    <p>Pixels with the label <code>void</code> should neither be considered in your loss nor in the accuracy of your segmentation. See implementation for details.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = 'https://i2dl.vc.in.tum.de/static/data/segmentation_data.zip'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(i2dl_exercises_path, 'datasets','segmentation')\n",
    "\n",
    "\n",
    "download_dataset(\n",
    "    url=download_url,\n",
    "    data_dir=data_root,\n",
    "    dataset_zip_name='segmentation_data.zip',\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "train_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/train.txt')\n",
    "val_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/val.txt')\n",
    "test_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to implement data augmentation methods, make yourself familiar with the segmentation dataset and how we implemented the `SegmentationData` class in `exercise_code/data/segmentation_dataset.py`. Furthermore, you can check the original label description in `datasets/segmentation/segmentation_data/info.html`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>WARNING</h3>\n",
    "    <p>While you can use data augmentation techniques during training, please do not implement any data normalization / augmentation in the directly in the dataset class, since we will be using our implementation on the the server to prevent cheating, by changing the labels for example ;)!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's look at a few samples of our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Img size: \", train_data[0][0].size())\n",
    "print(\"Segmentation size: \", train_data[0][1].size())\n",
    "\n",
    "num_example_imgs = 4\n",
    "plt.figure(figsize=(10, 5 * num_example_imgs))\n",
    "for i, (img, target) in enumerate(train_data[:num_example_imgs]):\n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 1)\n",
    "    plt.imshow(img.numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 2)\n",
    "    plt.imshow(label_img_to_rgb(target.numpy()))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dataset is considerably smaller when compared to our previous datasets. For instance, CIFAR10 consisted of thousands of images, whereas in this case, we only have 276 training images. Moreover, the task itself is significantly more challenging than a \"simple 10 class classification\" because we need to assign a label to every individual pixel. What's more, the images are much bigger as we are now considering images of size 240x240 instead of 32x32. \n",
    "\n",
    "Given these factors, it's important to set realistic expectations regarding the performance of our networks, so don't be too disappointed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Semantic Segmentation \n",
    "\n",
    "## Dummy Model\n",
    "\n",
    "In `exercise_code/networks/segmentation_nn.py` we define a naive `DummySegmentationModel`, which always predicts the scores of segmentation labels of the first image. Let's try it on a few images and visualize the outputs using the `visualizer` we provide. The `visualizer` takes in the model and dataset, and visualizes the first three (Input, Target, Prediction) pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dummy_model = DummySegmentationModel(target_image=train_data[0][1])\n",
    "\n",
    "# Visualization function\n",
    "visualizer(dummy_model, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can use the visualizer function in your training scenario to print out your model predictions on a regular basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Metrics\n",
    "In the context of image segmentation, the loss function employed is pixel-wise cross entropy loss. This particular loss function operates at the pixel level, considering each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector. \n",
    "<img src='https://i2dl.vc.in.tum.de/static/images/exercise_10/loss_img.png' width=80% height=80%/>\n",
    "source: https://www.jeremyjordan.me/semantic-segmentation/\n",
    "\n",
    "Up until now we only used the default loss function (`nn.CrossEntropyLoss`) in our solvers. However, In order to ignore the `unlabeled` pixels for the computation of our loss, we need to use a customized version of the loss for the initialization of our segmentation solver. The `ignore_index` argument of the loss can be used to filter the `unlabeled` pixels and computes the loss only over remaining pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "\n",
    "for (inputs, targets) in train_data[0:4]:\n",
    "    inputs, targets = inputs, targets\n",
    "    outputs = dummy_model(inputs.unsqueeze(0))\n",
    "    losses = loss_func(outputs, targets.unsqueeze(0)).item()\n",
    "    print(\"Loss:\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Non-zero loss for the first sample</h3> \n",
    "    <p>The output of our dummy model is one-hot-coded tensor. Since there is <b>softmax</b> function in the <b>nn.CrossEntropyLoss</b> function, the loss is:  \n",
    "    $$loss(x, class) = - \\log \\left( \\frac{\\exp(x[class])}{\\Sigma_j \\exp (x[j])} \\right) = âˆ’x[class]+\\log \\left( \\Sigma_j \\exp(x[j]) \\right)$$\n",
    "     and the loss will not be zero.    </p>\n",
    "<p>i.e. for $x=[0, 0, 0, 1],class=3$,$\\quad$ the loss: \n",
    "$loss(x,class) = -1 +\\log(\\exp(0)+\\exp(0)+\\exp(0)+\\exp(1)) = 0.7437$ </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To obtain an evaluation accuracy, we can simply compute the average per pixel accuracy of our network for a given image. We will use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    test_scores = []\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        targets_mask = (targets >= 0).cpu()\n",
    "        test_scores.append(np.mean((preds.cpu() == targets.cpu())[targets_mask].numpy()))\n",
    "\n",
    "    return np.mean(test_scores)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1,shuffle=False,num_workers=0)\n",
    "print(evaluate_model(dummy_model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You will see reasonably high numbers as your accuracy when you do the training later. The reason behind that is the fact that most output pixels are of a single class and the network can just overfit to common classes such as \"grass\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1: Design your own model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement your network architecture in <code>exercise_code/networks/segmentation_nn.py</code>. In this task, you will use pytorch to setup your model.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "To compensate for the dimension reduction of a typical convolution layer, you should probably include either a single `nn.Upsample` layer, use a combination of upsampling layers as well as convolutions or even transposed convolutions near the end of your network to get back to the target image shape.\n",
    "\n",
    "This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically). \n",
    "The only rules your model design has to follow are:\n",
    "* Perform the forward pass in `forward()`. Input dimension is (N, C, H, W) and output dimension is (N, num_classes, H, W)\n",
    "* Have less than 5 million parameters\n",
    "* Have a model size of less than 50MB after saving\n",
    "\n",
    "Furthermore, you need to pass all your hyperparameters to the model in a single dict `hparams`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Transfer learning</h3>\n",
    "    <p>In this exercise, we encourage you to do transfer learning as we learned in exercise 8, since this will boost your model performance and save training time. You can import pretrained models from torchvision in your model and use its feature extractor (e.g. <code>alexnet.features</code>) to get the image feature. Feel free to choose more advanced pretrained model like ResNet, MobileNet for your architecture design.</p>       \n",
    "</div>\n",
    "\n",
    "See [here](https://pytorch.org/vision/stable/models.html) for more info of the torchvison pretrained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # TODO: if you have any model arguments/hparams, define them here and read them from this dict inside SegmentationNN class\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegmentationNN(hp = hparams)\n",
    "test_seg_nn(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train your own model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> In addition to the network itself, you will also need to write the code for the model training.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from exercise_code.networks.segmentation_nn import SegmentationNN\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "model = SegmentationNN(hp=hparams)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Training on GPU\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Training on MPS - CUDA equivalent for Apple Silicon Chips\")\n",
    "    print(\"Note: MPS is not fully supported by PyTorch yet and might lead to errors.\")\n",
    "    print(\"If you encounter any issues, please manually switch to CPU.\")\n",
    "    print(\"In some cases with MPS, it is better to use 0 workers for dataloader.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Training on CPU\")\n",
    "\n",
    "########################################################################\n",
    "# TODO - Train Your Model                                              #\n",
    "########################################################################\n",
    "\n",
    "  \n",
    "#######################################################################\n",
    "#                           END OF YOUR CODE                          #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Test your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.device('cpu'))\n",
    "device = torch.device('cpu')\n",
    "test(evaluate_model(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualizer(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save the Model for Submission\n",
    "\n",
    " Once you are satisfied with the training of your model, you need to save it for submission. To be eligible for the bonus, you must attain an accuracy exceeding __64%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "save_model(model, \"segmentation_nn.model\")\n",
    "checkSize(path = \"./models/segmentation_nn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.util.submit import submit_exercise\n",
    "\n",
    "submit_exercise('../output/exercise10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Congratulations! You've just built your first semantic segmentation model with PyTorch! To complete the exercise, submit your final model to [the submission server](https://i2dl.vc.in.tum.de/) - you probably know the procedure by now.\n",
    "\n",
    "\n",
    "# Submission Goals\n",
    "\n",
    "- Goal: Implement and train a convolutional neural network for Semantic Segmentation.\n",
    "- Passing Criteria: Reach **Accuracy >= 64%** on __our__ test dataset. The submission system will show you your score after you submit.\n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fe789aef31382a17f319d0c537212ce4ecaf1c4433ef69f8b76775a6505725b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
