{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9CALuVmNMNkM"
   },
   "source": [
    "# Autoencoder for MNIST\n",
    "\n",
    "Welcome to this notebook where you'll be training an autoencoder using the MNIST dataset, which comprises handwritten digits. This exercise is the last where we present you with a structured skeleton to work with. However, in following exercises, we will only provide you with the dataset, task, and a test scenario, enabling you to test your skills and compete with your peers on our leaderboards. Get ready to dive in and showcase your deep learning expertise!\n",
    "\n",
    "\n",
    "## Your task:\n",
    "\n",
    "Autoencoders have various applications, including unsupervised pretraining using unlabeled data, followed by fine-tuning the encoder with labeled data. This approach can greatly enhance performance when there is only a little amount of labeled data but a lot of unlabeled data available.\n",
    "\n",
    "In this exercise, you will use the MNIST dataset, consisting of 60,000 images of handwritten digits. However, not all the image labels are available to you. Your first objective is to train an autoencoder to accurately reproduce these unlabeled images.\n",
    "\n",
    "Afterwards, you will transfer the weights of the pretrained encoder and perform fine-tuning on a classifier using the available labeled data. This technique is commonly known as **transfer learning**, which allows you to leverage the knowledge gained from the autoencoder to improve the classification of the handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XcU9f4APMNkT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# For automatic file reloading as usual\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb9dFU2EMNkW"
   },
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TRr4E4YVMNkW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\nimport os\\ngdrive_path='/content/gdrive/MyDrive/i2dl/exercise_08'\\n\\n# This will mount your google drive under 'MyDrive'\\ndrive.mount('/content/gdrive', force_remount=True)\\n# In order to access the files in this notebook we have to navigate to the correct folder\\nos.chdir(gdrive_path)\\n# Check manually if all files are present\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_08) is given.\n",
    "\n",
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_08'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JzDQg-kDMNkY"
   },
   "source": [
    "### Set up PyTorch environment in colab\n",
    "- Enable GPU via Runtime --> Change runtime type --> GPU\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hEDWAZ7-ZA4E"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dJCiVLV5o9QO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from exercise_code.image_folder_dataset import ImageFolderDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from exercise_code.tests.base_tests import bcolors\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dvaj6myXS7nN"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Google Colab</h3>\n",
    "    <p>\n",
    "In case you don't have a GPU, you can run this notebook on Google Colab where you can access a GPU for free, but, of course, you can also run this notebook on your CPU.\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VWgm75NnS9hr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the following device:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('You are using the following device: ', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm_rTAPnpsUo"
   },
   "source": [
    "## Setup TensorBoard\n",
    "\n",
    "In the previous exercise (Exercise 07), you learned how to use TensorBoard effectively. Let's use it again to enhance the convenience of debugging your network and the training process. Throughout this notebook, feel free to implement additional logs or visualizations into your TensorBoard, further improving your analysis and understanding of the network's behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QbAJFyHkMNke"
   },
   "outputs": [],
   "source": [
    "################# COLAB ONLY #################\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./ --port 6006\n",
    "\n",
    "# Use the cmd for less trouble, if you can. From the working directory, run: tensorboard --logdir=./ --port 6006"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "t-Yt2KRiMNkf"
   },
   "source": [
    "# 1. The MNIST Dataset\n",
    "\n",
    "First, let's download the MNIST dataset. As mentioned at the beginning of this notebook, MNIST is a dataset of 60,000 images depicting handwritten digits. However, labeling such a large dataset can be a costly process, leaving us in a challenging situation.\n",
    "\n",
    "To overcome this, a practical approach is to label a small subset of the images. Let's consider a scenario where you have hired another student to perform the labeling task for you. After some time, you have been provided with 300 labeled images. Out of these, 100 images will be used for training, another 100 for validation, and the remaining 100 for testing. Undoubtedly, this poses a challenge due to the limited number of labeled samples.\n",
    "\n",
    "Now, you have the flexibility to define any transforms that you deem necessary, either at this point or at a later stage. However, it's important to note that during the final evaluation on the server, no transformations will be applied to the test set.\n",
    "\n",
    "Feel free to experiment with various transforms as you proceed (you can also pass without any transforms). \n",
    "\n",
    "\n",
    "**Note**: We do **not** apply any transformations or normalization to the test set at the time of final evaluation on our server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "U5_eopjbMNkf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n",
      "'--force_download=True'\n",
      "https://i2dl.vc.in.tum.de/static/data/mnist.zip\n",
      "Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n",
      "'--force_download=True'\n",
      "https://i2dl.vc.in.tum.de/static/data/mnist.zip\n",
      "Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n",
      "'--force_download=True'\n",
      "https://i2dl.vc.in.tum.de/static/data/mnist.zip\n",
      "Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n",
      "'--force_download=True'\n",
      "https://i2dl.vc.in.tum.de/static/data/mnist.zip\n",
      "Found dataset folder. Skipped downloading. If you face issues, please re-download the dataset using\n",
      "'--force_download=True'\n",
      "https://i2dl.vc.in.tum.de/static/data/mnist.zip\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([])\n",
    "\n",
    "########################################################################\n",
    "# TODO: Feel free to define transforms (Only data augmentation)        #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "#transform = transforms.Compose([\n",
    "#    transforms.RandomRotation(180),            \n",
    "#])\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "mnist_root = os.path.join(i2dl_exercises_path, \"datasets\", \"mnist\")\n",
    "\n",
    "train_100_dataset = ImageFolderDataset(root=mnist_root,images='train_images.pt',labels='train_labels.pt',force_download=False,verbose=True,transform=transform)\n",
    "val_100_dataset = ImageFolderDataset(root=mnist_root,images='val_images.pt',labels='val_labels.pt',force_download=False,verbose=True,transform=transform)\n",
    "test_100_dataset = ImageFolderDataset(root=mnist_root,images='test_images.pt',labels='test_labels.pt',force_download=False,verbose=True,transform=transform)\n",
    "\n",
    "# We also set up the unlabeled images which we will use later\n",
    "unlabeled_train = ImageFolderDataset(root=mnist_root,images='unlabeled_train_images.pt',force_download=False,verbose=True,transform=transform)\n",
    "unlabeled_val = ImageFolderDataset(root=mnist_root,images='unlabeled_val_images.pt',force_download=False,verbose=True,transform=transform)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zwrT1ckAMNkg"
   },
   "source": [
    "The dataset consists of tuples of 28x28 pixel PIL images and a label that is an integer from 0 to 9. \n",
    "\n",
    "Let's turn a few of the images into numpy arrays, to look at their shape and visualize them and see if the labels we paid for are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "k7ct1J2CMNkh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our greyscale images:  (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (16,16) # Make the figures a bit bigger\n",
    "\n",
    "indices_arr = [83, 98, 92, 99, 78, 97, 90, 95, 93, 96]\n",
    "for i, index in enumerate(indices_arr):\n",
    "    image = np.array(train_100_dataset[index][0].squeeze()) # get the image of the data sample\n",
    "    label = train_100_dataset[index][1] # get the label of the data sample\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.imshow(image, cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(label))\n",
    "    \n",
    "plt.tight_layout()\n",
    "print('The shape of our greyscale images: ', image.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "V9sz_lHyqJoj"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Starting Simple</h3>\n",
    "    <p>\n",
    "Regardless of the size of the dataset, the first step is to evaluate the performance of a simple classifier. It is advisable to always start with a straightforward approach when tackling a problem and gradually build upon it to determine which changes yield improvements.</p>\n",
    "</div>\n",
    "\n",
    "# 2. A Simple Classifier\n",
    "\n",
    "In `exercise_code/models.py`, we prepared all classes for you, which you will finalize throughout the notebook to build an Autoencoder and an image classifier with PyTorch.\n",
    "\n",
    "<!-- In case image does not show, uncomment the following:\n",
    " ![network_split](img/network_split.png) \n",
    " -->\n",
    "<img name=\"network_split\" src=\"https://i2dl.vc.in.tum.de/static/images/exercise_08/network_split.png\">\n",
    "\n",
    "\n",
    "## 2.1 The Encoder\n",
    "\n",
    "Unlike previous models, we are going to split up the model into two parts: the `encoder` and the `classifier`. The `classifier` has a fixed task, generating predictions given a one-dimensional input. On the other hand, the `encoder`'s task is to extract meaningful information from the input, enabling the classifier to make accurate decisions. \n",
    "\n",
    "For now, both networks will be similar in design and consist of linear layers coupled with auxiliary layers. This split-up will be relevant later, e.g., by using convolutional layers, which are introduced in the lecture. We are going to set up the `encoder` now. \n",
    "\n",
    "Think about a good network architecture. You have complete freedom in this regard and can devise any network structure you think might be fitting. (\\*)\n",
    "\n",
    "Have a look at the documentation of `torch.nn` at https://pytorch.org/docs/stable/nn.html to learn how to use this module in order to build your network!\n",
    "\n",
    "Then implement your architecture: initialize it in `__init__()` and assign it to `self.model`. This is particularly easy using `nn.Sequential()`, where you only have to pass the list of your layers. \n",
    "\n",
    "To make your model customizable and support parameter search, do not use hardcoded hyperparameters - instead, pass them as a simple dictionary `hparams` (here, `n_hidden` is the number of neurons in the hidden layer) when initializing `models`.\n",
    "\n",
    "Here is a simple example:\n",
    "\n",
    "```python\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, self.hparams[\"n_hidden\"]),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(self.hparams[\"n_hidden\"], num_classes)\n",
    "        )\n",
    "```\n",
    "\n",
    "Have a look at the forward path in `forward(self, x)`, which is so easy that you don't need to implement it yourself.\n",
    "\n",
    "As PyTorch automatically computes the gradients, that's all you need to do! There is no need to manually calculate derivatives for the backward paths anymore! :)\n",
    "\n",
    "\n",
    "____\n",
    "\\* *The size of your final model must be less than 20 MB, which is approximately equivalent to 5 Mio. params. Note that this limit is quite lenient, you will probably need much fewer parameters!*\n",
    "\n",
    "*In order to keep things simple, you should only use fully connected layers for this task, as we need to revert the encoder architecture later on in the notebook.*\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>Encoder</code> class initialization in <code>exercise_code/models.py</code>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jNf7FrvwMNki"
   },
   "source": [
    "## 2.2 The Classifier\n",
    "\n",
    "Now let's implement the classifier. The classifier will utilize the encoder that you have defined in the above cell. By looking at `Classifier.forward`, you can see that we are essentially concatenating the `classifier`and the `encoder` together. Therefore, it is crucial to ensure that the input shape of the classifier matches the output shape of the encoder you implemented above\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>1. Implement the <code>Classifier</code> class network initialization in <code>exercise_code/models.py</code>.\n",
    "    </p>\n",
    "    <p>2. Define in the next cell your hyperparameters in a dictionary called 'hparams'.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "AawbvD1rMNkj"
   },
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "########################################################################\n",
    "# TODO: Define your hyper parameters here!                             #\n",
    "########################################################################\n",
    "hparams = {\n",
    "    'learning_rate': 3e-4,     \n",
    "    'batch_size': 2**10,            \n",
    "    'epochs': 60,           \n",
    "    'hidden_size': 300,          \n",
    "}\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bOYbUg8lAmgU"
   },
   "source": [
    "\n",
    "## 2.3 Optimizer\n",
    "Lastly, implement the function `set_optimizer` to define your optimizer. Here the documentation of `torch.optim` at https://pytorch.org/docs/stable/optim.html might be helpful.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>set_optimizer</code> method of the <code>Classifier</code> in <code>exercise_code/models.py</code>.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xrUfa-a7MNkk"
   },
   "source": [
    "## 2.4 Training & Validation Step\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p> Let's take a closer look at the training pipeline outlined below. It is explicitly written here in its entirety to provide you with a comprehensive understanding of its structure. Additionally, you can refer back to this pipeline whenever you encounter any uncertainties or need guidance.\n",
    " </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NY_lLaNWMNkk"
   },
   "outputs": [],
   "source": [
    " # One of the most crucial things in deep learning is to understand the training pipeline:\n",
    " # 1. Forward()          --> The forward pass of the network, to calculate the currnent loss.\n",
    " # 2. Backward()         --> The backward pass of the network, to calculate the gradients w.r.t the loss, calculated in the previous stage.\n",
    " # 3. Optimizer_step()   --> Update the weights w.r.t their corresponding gradients and the learnign rate.\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_classifier(classifier, train_loader, val_loader, loss_func, tb_logger, epochs=10, name=\"default\"):\n",
    "    \"\"\"\n",
    "    Train the classifier for a number of epochs.\n",
    "    \"\"\"\n",
    "    optimizer = classifier.optimizer\n",
    "    classifier = classifier.to(device)\n",
    "    validation_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        training_loss = 0\n",
    "        \n",
    "        # Training stage, where we want to update the parameters.\n",
    "        classifier.train()  # Set the model to training mode\n",
    "        \n",
    "        # Create a progress bar for the training loop.\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad() # Reset the gradients - VERY important! Otherwise they accumulate.\n",
    "            images, labels = batch # Get the images and labels from the batch, in the fashion we defined in the dataset and dataloader.\n",
    "            images, labels = images.to(device), labels.to(device) # Send the data to the device (GPU or CPU) - it has to be the same device as the model.\n",
    "\n",
    "            # Flatten the images to a vector. This is done because the classifier expects a vector as input.\n",
    "            # Could also be done by reshaping the images in the dataset.\n",
    "            images = images.view(images.shape[0], -1) \n",
    "\n",
    "            pred = classifier(images) # Stage 1: Forward().\n",
    "            loss = loss_func(pred, labels) # Compute the loss over the predictions and the ground truth.\n",
    "            loss.backward()  # Stage 2: Backward().\n",
    "            optimizer.step() # Stage 3: Update the parameters.\n",
    "\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(curr_train_loss = \"{:.8f}\".format(training_loss / (train_iteration + 1)), val_loss = \"{:.8f}\".format(validation_loss))\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            tb_logger.add_scalar(f'classifier_{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "            sleep(0.1) # Remove this line if you want to see the progress bar faster.\n",
    "\n",
    "        # Validation stage, where we don't want to update the parameters. Pay attention to the classifier.eval() line\n",
    "        # and \"with torch.no_grad()\" wrapper.\n",
    "        classifier.eval()\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                images = images.view(images.shape[0], -1) \n",
    "                pred = classifier(images)\n",
    "                loss = loss_func(pred, labels)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(val_loss = \"{:.8f}\".format(validation_loss / (val_iteration + 1)))\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                tb_logger.add_scalar(f'classifier_{name}/val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n",
    "                sleep(0.1) # Remove this line if you want to see the progress bar faster.\n",
    "        \n",
    "        # This value is used for the progress bar of the training loop.\n",
    "        validation_loss /= len(val_loader)\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KVKLlwlyMNkl"
   },
   "source": [
    "## 2.5 Fit Classification Model with Trainer\n",
    "Now it's finally time to train your model.\n",
    "Run the following cell to see the behold the magic of deep learning at play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "uBGavq9cMNkl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/60]: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00,  9.06it/s, curr_train_loss=2.30554485, val_loss=0.00000000]\n",
      "Validation Epoch [1/60]: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.45it/s, val_loss=2.30259752]\n",
      "Training Epoch [2/60]: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00,  7.72it/s, curr_train_loss=2.16484833, val_loss=2.30259752]\n",
      "Validation Epoch [2/60]: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.81it/s, val_loss=2.30268645]\n",
      "Training Epoch [3/60]: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00,  7.52it/s, curr_train_loss=2.06640053, val_loss=2.30268645]\n",
      "Validation Epoch [3/60]: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.56it/s, val_loss=2.30262351]\n",
      "Training Epoch [4/60]: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00,  7.38it/s, curr_train_loss=1.97857487, val_loss=2.30262351]\n",
      "Validation Epoch [4/60]: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.61it/s, val_loss=2.30191875]\n",
      "Training Epoch [5/60]: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00,  7.45it/s, curr_train_loss=1.89174342, val_loss=2.30191875]\n",
      "Validation Epoch [5/60]: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.82it/s, val_loss=2.30039525]\n",
      "Training Epoch [6/60]: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00,  8.32it/s, curr_train_loss=1.80302191, val_loss=2.30039525]\n",
      "Validation Epoch [6/60]: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.50it/s, val_loss=2.29783058]\n",
      "Training Epoch [7/60]: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00,  8.74it/s, curr_train_loss=1.71165192, val_loss=2.29783058]\n",
      "Validation Epoch [7/60]: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.42it/s, val_loss=2.29395223]\n",
      "Training Epoch [8/60]: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00,  8.85it/s, curr_train_loss=1.61726415, val_loss=2.29395223]\n",
      "Validation Epoch [8/60]: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.42it/s, val_loss=2.28877449]\n",
      "Training Epoch [9/60]: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00,  7.51it/s, curr_train_loss=1.52037013, val_loss=2.28877449]\n",
      "Validation Epoch [9/60]: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.55it/s, val_loss=2.28233957]\n",
      "Training Epoch [10/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  9.16it/s, curr_train_loss=1.42220676, val_loss=2.28233957]\n",
      "Validation Epoch [10/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.57it/s, val_loss=2.27443790]\n",
      "Training Epoch [11/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.81it/s, curr_train_loss=1.32407963, val_loss=2.27443790]\n",
      "Validation Epoch [11/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.58it/s, val_loss=2.26469064]\n",
      "Training Epoch [12/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.95it/s, curr_train_loss=1.22715771, val_loss=2.26469064]\n",
      "Validation Epoch [12/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.64it/s, val_loss=2.25295496]\n",
      "Training Epoch [13/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.99it/s, curr_train_loss=1.13183677, val_loss=2.25295496]\n",
      "Validation Epoch [13/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.50it/s, val_loss=2.23903894]\n",
      "Training Epoch [14/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  9.06it/s, curr_train_loss=1.03934801, val_loss=2.23903894]\n",
      "Validation Epoch [14/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.56it/s, val_loss=2.22281885]\n",
      "Training Epoch [15/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.88it/s, curr_train_loss=0.95067728, val_loss=2.22281885]\n",
      "Validation Epoch [15/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.66it/s, val_loss=2.20397115]\n",
      "Training Epoch [16/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.85it/s, curr_train_loss=0.86652774, val_loss=2.20397115]\n",
      "Validation Epoch [16/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.43it/s, val_loss=2.18247914]\n",
      "Training Epoch [17/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.75it/s, curr_train_loss=0.78682244, val_loss=2.18247914]\n",
      "Validation Epoch [17/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.44it/s, val_loss=2.15817189]\n",
      "Training Epoch [18/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  9.01it/s, curr_train_loss=0.71131730, val_loss=2.15817189]\n",
      "Validation Epoch [18/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.69it/s, val_loss=2.13038707]\n",
      "Training Epoch [19/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.80it/s, curr_train_loss=0.63935131, val_loss=2.13038707]\n",
      "Validation Epoch [19/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.75it/s, val_loss=2.09882259]\n",
      "Training Epoch [20/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.56it/s, curr_train_loss=0.57031202, val_loss=2.09882259]\n",
      "Validation Epoch [20/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.72it/s, val_loss=2.06343770]\n",
      "Training Epoch [21/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.58it/s, curr_train_loss=0.50440562, val_loss=2.06343770]\n",
      "Validation Epoch [21/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.97it/s, val_loss=2.02426624]\n",
      "Training Epoch [22/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.67it/s, curr_train_loss=0.44228980, val_loss=2.02426624]\n",
      "Validation Epoch [22/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.48it/s, val_loss=1.98179793]\n",
      "Training Epoch [23/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.65it/s, curr_train_loss=0.38494956, val_loss=1.98179793]\n",
      "Validation Epoch [23/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.70it/s, val_loss=1.93670928]\n",
      "Training Epoch [24/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.16it/s, curr_train_loss=0.33332208, val_loss=1.93670928]\n",
      "Validation Epoch [24/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.81it/s, val_loss=1.88960576]\n",
      "Training Epoch [25/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.71it/s, curr_train_loss=0.28816205, val_loss=1.88960576]\n",
      "Validation Epoch [25/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.68it/s, val_loss=1.84146607]\n",
      "Training Epoch [26/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.69it/s, curr_train_loss=0.24988660, val_loss=1.84146607]\n",
      "Validation Epoch [26/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.68it/s, val_loss=1.79312181]\n",
      "Training Epoch [27/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.61it/s, curr_train_loss=0.21834907, val_loss=1.79312181]\n",
      "Validation Epoch [27/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.57it/s, val_loss=1.74496627]\n",
      "Training Epoch [28/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  9.01it/s, curr_train_loss=0.19275373, val_loss=1.74496627]\n",
      "Validation Epoch [28/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.83it/s, val_loss=1.69734085]\n",
      "Training Epoch [29/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.60it/s, curr_train_loss=0.17200190, val_loss=1.69734085]\n",
      "Validation Epoch [29/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.70it/s, val_loss=1.65065551]\n",
      "Training Epoch [30/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  6.00it/s, curr_train_loss=0.15505394, val_loss=1.65065551]\n",
      "Validation Epoch [30/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.94it/s, val_loss=1.60563672]\n",
      "Training Epoch [31/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.57it/s, curr_train_loss=0.14108919, val_loss=1.60563672]\n",
      "Validation Epoch [31/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.80it/s, val_loss=1.56266618]\n",
      "Training Epoch [32/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.61it/s, curr_train_loss=0.12933752, val_loss=1.56266618]\n",
      "Validation Epoch [32/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.82it/s, val_loss=1.52216017]\n",
      "Training Epoch [33/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.53it/s, curr_train_loss=0.11931102, val_loss=1.52216017]\n",
      "Validation Epoch [33/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.86it/s, val_loss=1.48421967]\n",
      "Training Epoch [34/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.97it/s, curr_train_loss=0.11064822, val_loss=1.48421967]\n",
      "Validation Epoch [34/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.86it/s, val_loss=1.44869959]\n",
      "Training Epoch [35/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.75it/s, curr_train_loss=0.10303449, val_loss=1.44869959]\n",
      "Validation Epoch [35/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.75it/s, val_loss=1.41524601]\n",
      "Training Epoch [36/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.56it/s, curr_train_loss=0.09621032, val_loss=1.41524601]\n",
      "Validation Epoch [36/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.74it/s, val_loss=1.38373768]\n",
      "Training Epoch [37/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.67it/s, curr_train_loss=0.08991307, val_loss=1.38373768]\n",
      "Validation Epoch [37/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.80it/s, val_loss=1.35376883]\n",
      "Training Epoch [38/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.58it/s, curr_train_loss=0.08387651, val_loss=1.35376883]\n",
      "Validation Epoch [38/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.68it/s, val_loss=1.32515657]\n",
      "Training Epoch [39/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.63it/s, curr_train_loss=0.07790217, val_loss=1.32515657]\n",
      "Validation Epoch [39/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.84it/s, val_loss=1.29751384]\n",
      "Training Epoch [40/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  6.52it/s, curr_train_loss=0.07181099, val_loss=1.29751384]\n",
      "Validation Epoch [40/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.75it/s, val_loss=1.27054787]\n",
      "Training Epoch [41/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.61it/s, curr_train_loss=0.06548955, val_loss=1.27054787]\n",
      "Validation Epoch [41/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.49it/s, val_loss=1.24396658]\n",
      "Training Epoch [42/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.57it/s, curr_train_loss=0.05879088, val_loss=1.24396658]\n",
      "Validation Epoch [42/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.73it/s, val_loss=1.21733654]\n",
      "Training Epoch [43/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.74it/s, curr_train_loss=0.05170484, val_loss=1.21733654]\n",
      "Validation Epoch [43/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.82it/s, val_loss=1.19025648]\n",
      "Training Epoch [44/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.52it/s, curr_train_loss=0.04438169, val_loss=1.19025648]\n",
      "Validation Epoch [44/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.84it/s, val_loss=1.16249776]\n",
      "Training Epoch [45/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.66it/s, curr_train_loss=0.03707509, val_loss=1.16249776]\n",
      "Validation Epoch [45/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.75it/s, val_loss=1.13408315]\n",
      "Training Epoch [46/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.71it/s, curr_train_loss=0.03019313, val_loss=1.13408315]\n",
      "Validation Epoch [46/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.43it/s, val_loss=1.10549712]\n",
      "Training Epoch [47/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.60it/s, curr_train_loss=0.02432984, val_loss=1.10549712]\n",
      "Validation Epoch [47/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.83it/s, val_loss=1.07766020]\n",
      "Training Epoch [48/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.72it/s, curr_train_loss=0.01993170, val_loss=1.07766020]\n",
      "Validation Epoch [48/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.83it/s, val_loss=1.05187488]\n",
      "Training Epoch [49/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  6.93it/s, curr_train_loss=0.01701981, val_loss=1.05187488]\n",
      "Validation Epoch [49/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.57it/s, val_loss=1.02911198]\n",
      "Training Epoch [50/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.90it/s, curr_train_loss=0.01525799, val_loss=1.02911198]\n",
      "Validation Epoch [50/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.83it/s, val_loss=1.01017070]\n",
      "Training Epoch [51/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.66it/s, curr_train_loss=0.01422909, val_loss=1.01017070]\n",
      "Validation Epoch [51/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.73it/s, val_loss=0.99501961]\n",
      "Training Epoch [52/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.70it/s, curr_train_loss=0.01359527, val_loss=0.99501961]\n",
      "Validation Epoch [52/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.01it/s, val_loss=0.98346466]\n",
      "Training Epoch [53/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.87it/s, curr_train_loss=0.01313792, val_loss=0.98346466]\n",
      "Validation Epoch [53/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.91it/s, val_loss=0.97492141]\n",
      "Training Epoch [54/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.71it/s, curr_train_loss=0.01273274, val_loss=0.97492141]\n",
      "Validation Epoch [54/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.83it/s, val_loss=0.96860278]\n",
      "Training Epoch [55/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.75it/s, curr_train_loss=0.01231957, val_loss=0.96860278]\n",
      "Validation Epoch [55/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.91it/s, val_loss=0.96384019]\n",
      "Training Epoch [56/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.64it/s, curr_train_loss=0.01187499, val_loss=0.96384019]\n",
      "Validation Epoch [56/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.78it/s, val_loss=0.96010828]\n",
      "Training Epoch [57/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  8.83it/s, curr_train_loss=0.01139543, val_loss=0.96010828]\n",
      "Validation Epoch [57/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.74it/s, val_loss=0.95705467]\n",
      "Training Epoch [58/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.27it/s, curr_train_loss=0.01088781, val_loss=0.95705467]\n",
      "Validation Epoch [58/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.95it/s, val_loss=0.95442045]\n",
      "Training Epoch [59/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.64it/s, curr_train_loss=0.01036313, val_loss=0.95442045]\n",
      "Validation Epoch [59/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.43it/s, val_loss=0.95198691]\n",
      "Training Epoch [60/60]: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  7.58it/s, curr_train_loss=0.00983408, val_loss=0.95198691]\n",
      "Validation Epoch [60/60]: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.75it/s, val_loss=0.94971025]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "How did we do? Let's check the accuracy of the defaut classifier on the training and validation sets:\n",
      "Training Acc: 100.0%\n",
      "Validation Acc: 74.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.models import Classifier\n",
    "from exercise_code.models import Encoder\n",
    "\n",
    "hparams = {\n",
    "    'learning_rate': 3e-4,     \n",
    "    'batch_size': 2**10,            \n",
    "    'epochs': 60,           \n",
    "    'hidden_size': 500,          \n",
    "}\n",
    "\n",
    "\n",
    "# Create the encoder and the classifier.\n",
    "encoder = Encoder(hparams).to(device)\n",
    "classifier = Classifier(hparams, encoder).to(device)\n",
    "\n",
    "# Create a tensorboard logger.\n",
    "# NOTE: In order to see the logs, run the following command in the terminal: tensorboard --logdir=./\n",
    "# Also, in order to reset the logs, delete the logs folder MANUALLY.\n",
    "\n",
    "path = os.path.join('logs', 'cls_logs')\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the classifier.\n",
    "labled_train_loader = torch.utils.data.DataLoader(train_100_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "labled_val_loader = torch.utils.data.DataLoader(val_100_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "\n",
    "epochs = hparams.get('epochs', 10)\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "train_classifier(classifier, labled_train_loader, labled_val_loader, loss_func, tb_logger, epochs=epochs, name=\"Default\")\n",
    "\n",
    "print(\"Finished training!\")\n",
    "print(\"How did we do? Let's check the accuracy of the defaut classifier on the training and validation sets:\")\n",
    "print(f\"Training Acc: {classifier.getAcc(labled_train_loader)[1] * 100}%\")\n",
    "print(f\"Validation Acc: {classifier.getAcc(labled_val_loader)[1] * 100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i16vmHZXMNkm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Autoencoder\n",
    "\n",
    "With only a limited number of labeled images, it's challenging to achieve high performance. We have no money left to pay the student to create more labels, and labeling the data ourselves is out of question. A commonly used approach would be to apply data augmentation to maximize the potential of our limited labeled data, but here we provide another way to solve this problem: **transfer learning**.\n",
    "\n",
    "For each input, the autoencoder tries to reproduce the same image as an output. The difficulty behind this task is that the autoencoder has to go through a low dimensional bottleneck, which is called the **latent space**.\n",
    "In other words, the autoencoder learns to represent all the input information in a low dimensional latent space - it learns to compress the input distribution. To train the autoencoder, we use the mean squared error loss, which calculates the discrepancy between the input pixels and the output pixels. The best part is that this loss function doesn't require any labels!\n",
    "\n",
    "By pretraining the autoencoder in this way on a large amount of unlabeled data, we can capture valuable latent representations of the input images. Then, we transfer the weights of the encoder to our classifier, enabling it to benefit from the knowledge learned during the unsupervised pretraining phase. \n",
    "\n",
    "<!-- In case the image does not show, uncomment the following:\n",
    "![autoencoder](img/autoencoder.png) \n",
    "-->\n",
    "<img name=\"autoencoder\" src=\"https://i2dl.vc.in.tum.de/static/images/exercise_08/autoencoder.png\">\n",
    "\n",
    "After this, our encoder has learned to extract meaningful information from the inputs. We can then transfer its weights\n",
    "to a classifier architecture and finetune it with our labeled data, i.e., instead of initializing our encoder randomly, we are re-using the weights of our trained encoder from our autoencoder network. \n",
    "\n",
    "<!-- In case the image does not show, uncomment the following:\n",
    "![autoencoder_pretrained](img/pretrained.png) \n",
    "-->\n",
    "<img name=\"autoencoder_pretrained\" src=\"https://i2dl.vc.in.tum.de/static/images/exercise_08/pretrained.png\">\n",
    "\n",
    "\n",
    "Before we can train our autoencoder, you have to initialize your `decoder` architecture. The simplest way is to mirror your encoder architecture, which ensures that the `latent space` output of our `encoder` is correctly transformed to our input shape.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>Decoder</code> class initialization in <code>exercise_code/models.py</code>.</p>\n",
    "    <p>Implement <code>forward</code>, <code>set_optimizer</code>, <code>training_step</code> and <code>validation_step</code> of the <code>Autoencoder</code> in  <code>exercise_code/models.py</code>, following the pipeline we've shown you in train_classifier().</p>\n",
    "    <p>Note the differences between the classification task and now the regression task!</p>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "## 3.2 Autoencoder Training\n",
    "\n",
    "Now, we can train the full autoencoder consisting of both en- and decoder.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqqdoLDgMNkm"
   },
   "outputs": [],
   "source": [
    "from exercise_code.models import Autoencoder, Encoder, Decoder\n",
    "\n",
    "########################################################################\n",
    "# TODO: Define your hyperparameters here!                              #\n",
    "# Hint: use a large batch_size                                         #\n",
    "########################################################################\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "encoder_pretrained = Encoder(hparams).to(device)\n",
    "decoder = Decoder(hparams).to(device)\n",
    "autoencoder = Autoencoder(hparams, encoder_pretrained, decoder).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uRuIIm8YMNkn"
   },
   "source": [
    "Some tests to check whether we'll accept your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoAaC-NqMNkn"
   },
   "outputs": [],
   "source": [
    "from exercise_code.Util import printModelInfo, load_model\n",
    "_ = printModelInfo(autoencoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "plQwnphtqggl"
   },
   "source": [
    "After implementing the relevant functions - read the following code, and then run it.\n",
    "Keep in mind that an epoch here will take much longer since\n",
    "we are iterating through 5,8600 images instead of just 100.\n",
    "\n",
    "For speed, colab is indeed recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uuzXMq6zjbb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_pretrained = Encoder(hparams).to(device)\n",
    "decoder = Decoder(hparams).to(device)\n",
    "autoencoder = Autoencoder(hparams, encoder_pretrained, decoder).to(device)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=10, name='Autoencoder'):\n",
    "    \n",
    "    optimizer = model.optimizer\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs * len(train_loader) / 5, gamma=0.7)\n",
    "    validation_loss = 0\n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Train\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch}/{epochs}]')\n",
    "        training_loss = 0\n",
    "        for train_iteration, batch in training_loop:\n",
    "            \n",
    "            loss = model.training_step(batch, loss_func) # You need to implement this function.\n",
    "            training_loss += loss.item()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(train_loss = \"{:.8f}\".format(training_loss / (train_iteration + 1)), val_loss = \"{:.8f}\".format(validation_loss))\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            tb_logger.add_scalar(f'{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        # Validation\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch}/{epochs}]')\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                loss = model.validation_step(batch, loss_func) # You need to implement this function.\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(val_loss = \"{:.8f}\".format(validation_loss / (val_iteration + 1)))\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                tb_logger.add_scalar(f'{name}/val_loss', validation_loss / (val_iteration + 1), epoch * len(val_loader) + val_iteration)\n",
    "        # This value is for the progress bar of the training loop.\n",
    "        validation_loss /= len(val_loader)\n",
    "\n",
    "# Create a tensorboard logger.\n",
    "# NOTE: In order to see the logs, run the following command in the terminal: tensorboard --logdir=./\n",
    "# Also, in order to reset the logs, delete the logs folder MANUALLY.\n",
    "\n",
    "path = os.path.join('logs', 'ae_logs')\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the classifier.\n",
    "unlabled_train_loader = torch.utils.data.DataLoader(unlabeled_train, batch_size=hparams['batch_size'], shuffle=True)\n",
    "unlabled_val_loader = torch.utils.data.DataLoader(unlabeled_val, batch_size=hparams['batch_size'], shuffle=False)\n",
    "\n",
    "epochs = hparams.get('epochs', 5)\n",
    "loss_func = nn.MSELoss() # The loss function we use for regression (Could also be nn.L1Loss()).\n",
    "train_model(autoencoder, unlabled_train_loader, unlabled_val_loader, loss_func, tb_logger, epochs=epochs, name='Autoencoder')\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vdgiYWy4MNkq"
   },
   "source": [
    "Once trained, let's have a look at the reconstructed validation images (If you have not already looked at them in TensorBoard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a991mKcyMNkq"
   },
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.getReconstructions(unlabled_val_loader)\n",
    "for i in range(64):\n",
    "    plt.subplot(8,8,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(reconstructions[i], cmap='gray', interpolation='none')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "R2hrP5b1MNkr"
   },
   "source": [
    "# 4. Transfer Learning\n",
    "\n",
    "## 4.1 The pretrained Classifier\n",
    "\n",
    "Now we initialize another classifier but this time with the pretrained encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OELYQAUmMNkr"
   },
   "outputs": [],
   "source": [
    "from exercise_code.models import Classifier\n",
    "from copy import deepcopy\n",
    "\n",
    "hparams = {}\n",
    "########################################################################\n",
    "# TODO: Define your hyperparameters here!                             #\n",
    "########################################################################\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "\n",
    "encoder_pretrained_copy = deepcopy(encoder_pretrained)\n",
    "classifier_pretrained = Classifier(hparams, encoder_pretrained_copy).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "R8FUtih6MNks"
   },
   "source": [
    "Let's define another trainer that will utilize the pretrained classifier, allowing us to compare its performance with the classifier trained only on the labeled data. To achieve a reasonable result, you may need to optimize the parameters you defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx_euorWMNks"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a tensorboard logger.\n",
    "# NOTE: In order to see the logs, run the following command in the terminal: tensorboard --logdir=./\n",
    "# Also, in order to reset the logs, delete the logs folder MANUALLY.\n",
    "# Pay attention that if you run this cell mutltiple times, the pretrained_encoder\n",
    "# is not reset, and will keep training from where it stopped. Thus, it could overfit.\n",
    "\n",
    "path = os.path.join('logs', 'pretrained_cls_logs')\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "batch_size = hparams.get('batch_size', 16)\n",
    "labled_train_loader = torch.utils.data.DataLoader(train_100_dataset, batch_size=batch_size, shuffle=True)\n",
    "labled_val_loader = torch.utils.data.DataLoader(val_100_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "epochs = hparams.get('epochs', 20)\n",
    "loss_func = nn.CrossEntropyLoss() # The loss function we use for classification.\n",
    "train_classifier(classifier_pretrained, labled_train_loader, labled_val_loader, loss_func, tb_logger, epochs=epochs, name='Pretrained')\n",
    "\n",
    "print(\"Finished training!\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H-pm1MY_MNks"
   },
   "source": [
    "Let's have a look at the validation accuracy of the two different classifiers and compare them. And don't forget that you can also monitor your training in TensorBoard.\n",
    "\n",
    "We will only look at the test accuracy and compare our two classifiers with respect to that in the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-e5Bd9KLMNkt"
   },
   "outputs": [],
   "source": [
    "val_acc_scracth = classifier.getAcc(labled_val_loader)[1]*100\n",
    "color = 'green' if val_acc_scracth > 55 else 'red'\n",
    "print(f\"Validation accuracy when training from scratch: {bcolors.colorize(color, val_acc_scracth)}%\")\n",
    "\n",
    "val_acc_pretrained = classifier_pretrained.getAcc(labled_val_loader)[1]*100\n",
    "color = 'green' if val_acc_pretrained > 55 else 'red'\n",
    "print(f\"Validation accuracy with pretraining: {bcolors.colorize(color, val_acc_pretrained)}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zAp2OTyf4_5b"
   },
   "source": [
    "Now that everything is working, feel free to play around with different architectures. As you've seen, it's quite easy to define your model or do adpations there.\n",
    "\n",
    "To pass this submission, you will need to achieve an accuracy of **55%**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OmEYmRT-5S-e"
   },
   "source": [
    "# Save your model & Report Test Accuracy\n",
    "\n",
    "When you are finally done with your **hyperparameter tuning**, achieved **at least 55% validation accuracy** and are happy with your final model, you can save it here.\n",
    "\n",
    "Before that, please check again whether the number of parameters is below 5 Mio and the file size is below 20 MB.\n",
    "\n",
    "Once your final model is saved, we'll finally report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S69ETKxD5TcE"
   },
   "outputs": [],
   "source": [
    "from exercise_code.Util import test_and_save\n",
    "test_dl = torch.utils.data.DataLoader(test_100_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "test_acc = classifier.getAcc(test_dl)[1]*100\n",
    "color = 'green' if test_acc > 55 else 'red'\n",
    "print(f\"Test accuracy when training from scratch: {bcolors.colorize(color, test_acc)}%\")\n",
    "\n",
    "test_acc = classifier_pretrained.getAcc(test_dl)[1]*100\n",
    "color = 'green' if test_acc > 55 else 'red'\n",
    "print(f\"Test accuracy with pretraining: {bcolors.colorize(color, test_acc)}%\")\n",
    "\n",
    "test_and_save(classifier_pretrained, labled_val_loader, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enZCnGL6MNkt"
   },
   "outputs": [],
   "source": [
    "# Now zip the folder for upload\n",
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('../output/exercise_08')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7fuo3Tf9MNku",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Congratulations on completing your first autoencoder and successfully transferring the weights to a classifier! It's remarkable how much easier this process becomes with the power of PyTorch, compared to working with plain NumPy, right?\n",
    "\n",
    "To complete the exercise, please submit your final model to [our submission portal](https://i2dl.vc.in.tum.de/) - you should be already familiar with the submission procedure. Next, it is time to get started with some more complex neural networks and tasks in the upcoming exercises. See you next week!\n",
    "\n",
    "# Submission Goals\n",
    "\n",
    "- Goal: Successfully implement a fully connected autoencoder for MNIST with Pytorch and transfer the encoder weights to a classifier.\n",
    "\n",
    "- Passing Criteria: There are no unit tests that check specific components of your code. The only thing that's required to pass the submission, is your model to reach at least **55% accuracy** on __our__ test dataset. The submission system will show you a number between 0 and 100 which corresponds to your accuracy.\n",
    "\n",
    "\n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for the bonus."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "54970da6898dad277dbf355945c2dee7f942d2a31ec1fc1455b6d4f552d07b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
